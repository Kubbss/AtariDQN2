{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DQN for Atari Pong\n",
    "\n",
    "Clean implementation of Deep Q-Network (DQN) for PongNoFrameskip-v4\n",
    "\n",
    "**Run cells in order:**\n",
    "1. Imports\n",
    "2. Wrappers\n",
    "3. Model\n",
    "4. Replay Buffer\n",
    "5. DQN Agent\n",
    "6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper, ObservationWrapper, RewardWrapper\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)  # Disable OpenCL for reproducibility\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrappers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrappers defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Atari Environment Wrappers\n",
    "# Adapted from OpenAI Baselines for gymnasium\n",
    "\n",
    "class NoopResetEnv(Wrapper):\n",
    "    \"\"\"Sample initial states by taking random number of no-ops on reset.\"\"\"\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        if hasattr(self.env.unwrapped, 'np_random'):\n",
    "            noops = self.env.unwrapped.np_random.integers(1, self.noop_max + 1)\n",
    "        else:\n",
    "            noops = random.randint(1, self.noop_max)\n",
    "        \n",
    "        for _ in range(noops):\n",
    "            obs, _, terminated, truncated, _ = self.env.step(self.noop_action)\n",
    "            if terminated or truncated:\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(Wrapper):\n",
    "    \"\"\"Return only every `skip`-th frame, max-pooling over last 2 frames.\"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        for i in range(self._skip):\n",
    "            obs, reward, term, trunc, inf = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            terminated = terminated or term\n",
    "            truncated = truncated or trunc\n",
    "            info.update(inf)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class WarpFrame(ObservationWrapper):\n",
    "    \"\"\"Warp frames to 84x84 grayscale as done in the Nature paper.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(self.height, self.width), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(Wrapper):\n",
    "    \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.was_real_done = terminated or truncated\n",
    "        \n",
    "        # Check current lives\n",
    "        lives = info.get('lives', self.lives)\n",
    "        if lives < self.lives and lives > 0:\n",
    "            terminated = True\n",
    "        self.lives = lives\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            obs, _, _, _, info = self.env.step(0)  # No-op step\n",
    "        self.lives = info.get('lives', 0)\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "class ClipRewardEnv(RewardWrapper):\n",
    "    \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "    def reward(self, reward):\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class FrameStack(Wrapper):\n",
    "    \"\"\"Stack k last frames. Returns lazy array, which is much more memory efficient.\"\"\"\n",
    "    def __init__(self, env, k=4):\n",
    "        super().__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(k, *obs_shape), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return np.stack(list(self.frames), axis=0)\n",
    "\n",
    "\n",
    "def make_atari_env(env_id, max_episode_steps=None):\n",
    "    \"\"\"Create and configure Atari environment.\"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    if max_episode_steps:\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    \n",
    "    # Apply standard Atari wrappers\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = WarpFrame(env)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = FrameStack(env, k=4)\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "print(\"âœ… Wrappers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: DQN Model (CNN)\n",
    "\n",
    "class DQNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DQN CNN architecture from Nature paper.\n",
    "    Input: (batch, 4, 84, 84) - 4 stacked grayscale frames\n",
    "    Output: (batch, n_actions) - Q-values for each action\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After conv layers: (64, 7, 7) = 3136 features\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalize input to [0, 1]\n",
    "        x = x.float() / 255.0\n",
    "        \n",
    "        # Convolutional layers with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"âœ… Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "replay_buffer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Replay Buffer defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Experience Replay Buffer\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer for DQN.\n",
    "    Stores transitions and samples random batches for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack and convert to tensors\n",
    "        states = torch.FloatTensor(np.array([e.state for e in batch]))\n",
    "        actions = torch.LongTensor([e.action for e in batch])\n",
    "        rewards = torch.FloatTensor([e.reward for e in batch])\n",
    "        next_states = torch.FloatTensor(np.array([e.next_state for e in batch]))\n",
    "        dones = torch.FloatTensor([e.done for e in batch])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "print(\"âœ… Replay Buffer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dqn_agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DQN Agent defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: DQN Agent\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape,\n",
    "        n_actions,\n",
    "        device='cuda',\n",
    "        lr=0.0000625,  # Learning rate from Nature paper\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay=1000000,  # Decay over 1M steps\n",
    "        target_update_freq=10000,  # Update target network every 10k steps\n",
    "        buffer_size=1000000,  # 1M transitions\n",
    "        batch_size=32\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        self.q_network = DQNModel(n_actions).to(device)\n",
    "        self.target_network = DQNModel(n_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, eps=1.5e-4)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"Calculate current epsilon value.\"\"\"\n",
    "        if self.steps < self.epsilon_decay:\n",
    "            return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                   (1 - self.steps / self.epsilon_decay)\n",
    "        return self.epsilon_end\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.get_epsilon():\n",
    "            return random.randrange(self.n_actions)\n",
    "        \n",
    "        # Greedy action selection\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step using Double DQN.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        q_values = self.q_network(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN: use main network to select, target network to evaluate\n",
    "        with torch.no_grad():\n",
    "            next_q_values_main = self.q_network(next_states)\n",
    "            next_actions = next_q_values_main.argmax(1)\n",
    "            next_q_values_target = self.target_network(next_states)\n",
    "            next_q_value = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_value, target_q_value)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps = checkpoint['steps']\n",
    "\n",
    "\n",
    "print(\"âœ… DQN Agent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "training",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `PongNoFrameskip` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameNotFound\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 6: Training Loop\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Create environment\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m env = \u001b[43mmake_atari_env\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPongNoFrameskip-v4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m state_shape = env.observation_space.shape\n\u001b[32m      6\u001b[39m n_actions = env.action_space.n\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mmake_atari_env\u001b[39m\u001b[34m(env_id, max_episode_steps)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_atari_env\u001b[39m(env_id, max_episode_steps=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create and configure Atari environment.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_episode_steps:\n\u001b[32m    136\u001b[39m         env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\registration.py:681\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    680\u001b[39m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     env_spec = \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\registration.py:526\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(env_id)\u001b[39m\n\u001b[32m    520\u001b[39m     logger.warn(\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m     )\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m    528\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    529\u001b[39m     )\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\registration.py:392\u001b[39m, in \u001b[36m_check_version_exists\u001b[39m\u001b[34m(ns, name, version)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\envs\\registration.py:369\u001b[39m, in \u001b[36m_check_name_exists\u001b[39m\u001b[34m(ns, name)\u001b[39m\n\u001b[32m    366\u001b[39m namespace_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m suggestion_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error.NameNotFound(\n\u001b[32m    370\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[31mNameNotFound\u001b[39m: Environment `PongNoFrameskip` doesn't exist."
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Loop\n",
    "\n",
    "# Create environment\n",
    "env = make_atari_env('PongNoFrameskip-v4')\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"Environment: PongNoFrameskip-v4\")\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create agent\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    n_actions=n_actions,\n",
    "    device=device,\n",
    "    lr=0.0000625,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay=1000000,\n",
    "    target_update_freq=10000,\n",
    "    buffer_size=1000000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "total_steps = 10_000_000  # 10M steps\n",
    "learning_starts = 50_000  # Start learning after 50k steps\n",
    "train_freq = 4  # Train every 4 steps\n",
    "save_freq = 500_000  # Save every 500k steps\n",
    "eval_freq = 100_000  # Evaluate every 100k steps\n",
    "\n",
    "# Statistics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "current_episode_reward = 0\n",
    "current_episode_length = 0\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = \"models/dqn_pong.pth\"\n",
    "\n",
    "# Initialize environment\n",
    "state, info = env.reset()\n",
    "print(f\"\\nâœ… Starting training from step 0\")\n",
    "print(f\"   Total steps: {total_steps:,}\")\n",
    "print(f\"   Learning starts at: {learning_starts:,} steps\")\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "    # Select action\n",
    "    action = agent.select_action(state, training=True)\n",
    "    \n",
    "    # Take step\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Store transition\n",
    "    agent.store_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Update statistics\n",
    "    current_episode_reward += reward\n",
    "    current_episode_length += 1\n",
    "    \n",
    "    # Train\n",
    "    if step >= learning_starts and step % train_freq == 0:\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            pbar.set_postfix({\n",
    "                'epsilon': f'{agent.get_epsilon():.3f}',\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'avg_reward': f'{np.mean(episode_rewards[-10:]):.1f}' if len(episode_rewards) >= 10 else 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Handle episode end\n",
    "    if done:\n",
    "        episode_rewards.append(current_episode_reward)\n",
    "        episode_lengths.append(current_episode_length)\n",
    "        current_episode_reward = 0\n",
    "        current_episode_length = 0\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    # Save model periodically\n",
    "    if step > 0 and step % save_freq == 0:\n",
    "        agent.save(model_path)\n",
    "        print(f\"\\nðŸ’¾ Model saved at step {step:,}\")\n",
    "        if len(episode_rewards) >= 10:\n",
    "            recent_avg = np.mean(episode_rewards[-10:])\n",
    "            print(f\"   Recent avg reward: {recent_avg:.2f}\")\n",
    "    \n",
    "    # Evaluate and show progress\n",
    "    if step > 0 and step % eval_freq == 0 and len(episode_rewards) >= 10:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_length = np.mean(episode_lengths[-10:])\n",
    "        print(f\"\\nðŸ“Š Step {step:,}:\")\n",
    "        print(f\"   Avg reward (last 10): {avg_reward:.2f}\")\n",
    "        print(f\"   Avg episode length: {avg_length:.1f}\")\n",
    "        print(f\"   Epsilon: {agent.get_epsilon():.3f}\")\n",
    "        \n",
    "        # Progress indicators\n",
    "        if avg_reward > 10:\n",
    "            print(\"   ðŸŽ‰ðŸŽ‰ðŸŽ‰ EXCELLENT! Agent is winning!\")\n",
    "        elif avg_reward > 0:\n",
    "            print(\"   ðŸŽ‰ BREAKTHROUGH! Agent is winning!\")\n",
    "        elif avg_reward > -10:\n",
    "            print(\"   ðŸŽ¯ Great progress! Agent is learning!\")\n",
    "        elif avg_reward > -15:\n",
    "            print(\"   ðŸ“ˆ Starting to improve!\")\n",
    "        elif avg_reward > -19:\n",
    "            print(\"   ðŸ“Š Better than random!\")\n",
    "        else:\n",
    "            print(\"   â³ Still exploring...\")\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "# Final save\n",
    "agent.save(model_path)\n",
    "env.close()\n",
    "pbar.close()\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Total episodes: {len(episode_rewards)}\")\n",
    "if len(episode_rewards) > 0:\n",
    "    print(f\"Final average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Best average reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
