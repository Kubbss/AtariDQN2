{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:48:04.941417700Z",
     "start_time": "2025-12-12T17:48:04.061872100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pip setuptools wheel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86c69207428aa2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:48:09.919701600Z",
     "start_time": "2025-12-12T17:48:06.663746900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torch-2.9.1%2Bcu128-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torchvision-0.24.1%2Bcu128-cp310-cp310-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torchaudio-2.9.1%2Bcu128-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu128/torch-2.9.1%2Bcu128-cp310-cp310-win_amd64.whl (2862.1 MB)\n",
      "   ---------------------------------------- 0.0/2.9 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.9 GB 110.2 MB/s eta 0:00:26\n",
      "    --------------------------------------- 0.0/2.9 GB 114.0 MB/s eta 0:00:25\n",
      "    --------------------------------------- 0.1/2.9 GB 115.2 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.1/2.9 GB 115.8 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 0.1/2.9 GB 116.1 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 0.1/2.9 GB 116.4 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.2/2.9 GB 116.5 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.2/2.9 GB 117.8 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.2/2.9 GB 117.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.2/2.9 GB 117.8 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.3/2.9 GB 117.8 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 0.3/2.9 GB 118.0 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.3/2.9 GB 118.0 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 0.3/2.9 GB 118.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.4/2.9 GB 118.8 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.4/2.9 GB 118.9 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 0.4/2.9 GB 118.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.4/2.9 GB 118.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.5/2.9 GB 118.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 0.5/2.9 GB 118.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 0.5/2.9 GB 118.9 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.5/2.9 GB 118.9 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 0.6/2.9 GB 118.9 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.6/2.9 GB 118.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.6/2.9 GB 118.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 0.6/2.9 GB 118.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 0.7/2.9 GB 118.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 0.7/2.9 GB 118.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 0.7/2.9 GB 118.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 0.7/2.9 GB 118.8 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 0.8/2.9 GB 118.9 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 0.8/2.9 GB 118.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 0.8/2.9 GB 118.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 0.8/2.9 GB 118.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 0.9/2.9 GB 118.0 MB/s eta 0:00:18\n",
      "   ------------ --------------------------- 0.9/2.9 GB 118.1 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 0.9/2.9 GB 118.9 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 0.9/2.9 GB 118.9 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 1.0/2.9 GB 118.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 1.0/2.9 GB 118.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 1.0/2.9 GB 118.1 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 1.0/2.9 GB 118.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 1.1/2.9 GB 118.9 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 1.1/2.9 GB 118.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 1.1/2.9 GB 118.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 1.1/2.9 GB 118.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 1.2/2.9 GB 118.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 1.2/2.9 GB 118.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 1.2/2.9 GB 111.0 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 1.2/2.9 GB 108.1 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.2/2.9 GB 108.1 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.3/2.9 GB 108.1 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 1.3/2.9 GB 107.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 1.3/2.9 GB 107.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 1.3/2.9 GB 107.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 1.3/2.9 GB 108.1 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.9 GB 102.8 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.9 GB 101.6 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.9 GB 99.8 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.9 GB 100.3 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.5/2.9 GB 108.8 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 1.5/2.9 GB 107.5 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 1.5/2.9 GB 107.5 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.5/2.9 GB 107.5 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.5/2.9 GB 106.8 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 1.6/2.9 GB 105.4 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.9 GB 105.4 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.9 GB 110.3 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 1.6/2.9 GB 111.8 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 1.7/2.9 GB 114.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 1.7/2.9 GB 114.0 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.7/2.9 GB 114.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 1.7/2.9 GB 117.2 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 1.8/2.9 GB 115.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 1.8/2.9 GB 115.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 1.8/2.9 GB 116.4 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 1.8/2.9 GB 118.9 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.9/2.9 GB 118.9 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.9/2.9 GB 118.0 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.9/2.9 GB 118.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 1.9/2.9 GB 118.0 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.0/2.9 GB 118.0 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 2.0/2.9 GB 118.9 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.9 GB 118.9 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.9 GB 118.9 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 2.1/2.9 GB 118.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 2.1/2.9 GB 118.9 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 2.1/2.9 GB 118.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.9 GB 118.9 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.2/2.9 GB 118.9 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.2/2.9 GB 118.9 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.2/2.9 GB 118.9 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.2/2.9 GB 118.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.3/2.9 GB 118.0 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.3/2.9 GB 118.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.3/2.9 GB 118.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.3/2.9 GB 118.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.4/2.9 GB 118.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.4/2.9 GB 118.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.4/2.9 GB 118.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.4/2.9 GB 118.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.5/2.9 GB 118.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.5/2.9 GB 118.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.5/2.9 GB 118.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.5/2.9 GB 118.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.5/2.9 GB 107.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.6/2.9 GB 108.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.6/2.9 GB 108.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.6/2.9 GB 108.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.6/2.9 GB 108.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.7/2.9 GB 107.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.7/2.9 GB 107.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.7/2.9 GB 108.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.9 GB 108.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.8/2.9 GB 106.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.8/2.9 GB 106.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.8/2.9 GB 106.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 GB 104.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 GB 54.9 MB/s  0:00:27\n",
      "Downloading https://download.pytorch.org/whl/cu128/torchvision-0.24.1%2Bcu128-cp310-cp310-win_amd64.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 9.0/9.0 MB 112.3 MB/s  0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu128/torchaudio-2.9.1%2Bcu128-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 61.0 MB/s  0:00:00\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.3 MB/s  0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   --------------- ------------------------ 3/8 [fsspec]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------- -------------- 5/8 [torch]\n",
      "   ------------------------------ --------- 6/8 [torchvision]\n",
      "   ------------------------------ --------- 6/8 [torchvision]\n",
      "   ------------------------------ --------- 6/8 [torchvision]\n",
      "   ----------------------------------- ---- 7/8 [torchaudio]\n",
      "   ---------------------------------------- 8/8 [torchaudio]\n",
      "\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.9.1+cu128 torchaudio-2.9.1+cu128 torchvision-0.24.1+cu128\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11d76180279cb95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:51:44.327759500Z",
     "start_time": "2025-12-12T17:51:43.524465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[atari]\n",
      "  Using cached gymnasium-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from gymnasium[atari]) (1.23.5)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium[atari])\n",
      "  Using cached cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from gymnasium[atari]) (4.15.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Collecting ale_py>=0.9 (from gymnasium[atari])\n",
      "  Downloading ale_py-0.11.2-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Using cached gymnasium-1.2.2-py3-none-any.whl (952 kB)\n",
      "Downloading ale_py-0.11.2-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 3.1/3.5 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 18.7 MB/s  0:00:00\n",
      "Using cached cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, ale_py, gymnasium\n",
      "\n",
      "   ------------------------------ --------- 3/4 [gymnasium]\n",
      "   ------------------------------ --------- 3/4 [gymnasium]\n",
      "   ---------------------------------------- 4/4 [gymnasium]\n",
      "\n",
      "Successfully installed ale_py-0.11.2 cloudpickle-3.1.2 farama-notifications-0.0.4 gymnasium-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"gymnasium[atari]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db73391a608f635b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:52:33.073316700Z",
     "start_time": "2025-12-12T17:52:04.206693600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3[extra]\n",
      "  Using cached stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (1.2.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (1.23.5)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (2.9.1+cu128)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (3.10.8)\n",
      "Collecting opencv-python (from stable-baselines3[extra])\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pygame (from stable-baselines3[extra])\n",
      "  Downloading pygame-2.6.1-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (2.10.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (7.1.3)\n",
      "Collecting tqdm (from stable-baselines3[extra])\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (0.11.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from stable-baselines3[extra]) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3[extra]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.43.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.45.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->stable-baselines3[extra]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->stable-baselines3[extra]) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->stable-baselines3[extra]) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.3.1)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3[extra]) (1.17.0)\n",
      "Collecting numpy<3.0,>=1.20 (from stable-baselines3[extra])\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->stable-baselines3[extra])\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kubam\\miniconda3\\envs\\tf210\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Using cached stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Downloading pygame-2.6.1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/10.6 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 31.6 MB/s  0:00:00\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pygame, numpy, mdurl, opencv-python, markdown-it-py, rich, stable-baselines3\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "  Attempting uninstall: numpy\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "    Found existing installation: numpy 1.23.5\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "    Uninstalling numpy-1.23.5:\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "   ----- ---------------------------------- 1/8 [pygame]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [numpy]\n",
      "   -------------------- ------------------- 4/8 [opencv-python]\n",
      "   -------------------- ------------------- 4/8 [opencv-python]\n",
      "   ------------------------- -------------- 5/8 [markdown-it-py]\n",
      "   ------------------------------ --------- 6/8 [rich]\n",
      "   ----------------------------------- ---- 7/8 [stable-baselines3]\n",
      "   ---------------------------------------- 8/8 [stable-baselines3]\n",
      "\n",
      "Successfully installed markdown-it-py-4.0.0 mdurl-0.1.2 numpy-2.2.6 opencv-python-4.12.0.88 pygame-2.6.1 rich-14.2.0 stable-baselines3-2.7.1 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \"stable-baselines3[extra]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8156fcaa8935feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:54:29.154560500Z",
     "start_time": "2025-12-12T17:53:59.299457500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda available: True\n",
      "gpu: NVIDIA GeForce RTX 4070\n",
      "torch cuda version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "    print(\"torch cuda version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b7abb0163fa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:54:53.888932600Z",
     "start_time": "2025-12-12T17:54:52.617816800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape: (210, 160, 3)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py  # registers ALE envs\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\")  # no rendering needed for a smoke test\n",
    "obs, info = env.reset()\n",
    "print(\"obs shape:\", obs.shape)\n",
    "\n",
    "for _ in range(200):\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a764ac8a",
   "metadata": {},
   "source": [
    "# DQN Training Setup\n",
    "\n",
    "**To start training, run these cells in order:**\n",
    "1. Cell with Model class (torch imports + Model definition)\n",
    "2. Cell with ReplayBuffer class\n",
    "3. Cell with Environment Preprocessing (PreprocessAtari, FrameStack)\n",
    "4. Cell with DQNAgent class\n",
    "5. **Cell with Training Loop** - This is the one that actually starts training!\n",
    "\n",
    "**Training will:**\n",
    "- Run for 1,000,000 steps\n",
    "- Start learning after 10,000 steps of exploration\n",
    "- Save model every 100,000 steps to `models/dqn_pong.pth`\n",
    "- Show progress with epsilon, loss, and episode rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5770f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure torch is imported (if not already imported in earlier cells)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e1871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing wrappers updated with proper reset/step methods!\n"
     ]
    }
   ],
   "source": [
    "# FIXED: PreprocessAtari wrapper with proper reset and step methods\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import Wrapper\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "class PreprocessAtari(Wrapper):\n",
    "    \"\"\"\n",
    "    Preprocesses Atari frames: resize to 84x84, convert to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Convert to grayscale if needed\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Resize to 84x84\n",
    "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return self.observation(obs), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, terminated, truncated, info\n",
    "\n",
    "class FrameStack(Wrapper):\n",
    "    \"\"\"\n",
    "    Stacks the last n frames together.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_frames=4):\n",
    "        super().__init__(env)\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque(maxlen=n_frames)\n",
    "        \n",
    "        # Update observation space\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(n_frames, obs_shape[0], obs_shape[1]), \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Fill the frame stack with the first frame\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Stack frames: (n_frames, height, width)\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "print(\"âœ… Preprocessing wrappers updated with proper reset/step methods!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7835796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Named tuple for storing experiences\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer for DQN.\n",
    "    Stores and samples batches of experiences for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add an experience to the buffer.\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack the batch\n",
    "        states = torch.stack([e.state for e in batch])\n",
    "        actions = torch.tensor([e.action for e in batch], dtype=torch.long)\n",
    "        rewards = torch.tensor([e.reward for e in batch], dtype=torch.float32)\n",
    "        next_states = torch.stack([e.next_state for e in batch])\n",
    "        dones = torch.tensor([e.done for e in batch], dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import Wrapper\n",
    "import cv2\n",
    "\n",
    "class PreprocessAtari(Wrapper):\n",
    "    \"\"\"\n",
    "    Preprocesses Atari frames: resize to 84x84, convert to grayscale, normalize.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Convert to grayscale if needed\n",
    "        if len(obs.shape) == 3:\n",
    "            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Resize to 84x84\n",
    "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        return self.observation(obs), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, terminated, truncated, info\n",
    "\n",
    "class FrameStack(Wrapper):\n",
    "    \"\"\"\n",
    "    Stacks the last n frames together.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_frames=4):\n",
    "        super().__init__(env)\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque(maxlen=n_frames)\n",
    "        \n",
    "        # Update observation space\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(n_frames, obs_shape[0], obs_shape[1]), \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Fill the frame stack with the first frame\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Stack frames: (n_frames, height, width)\n",
    "        return np.stack(self.frames, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae6503",
   "metadata": {},
   "source": [
    "## ðŸš¨ Problem Diagnosis\n",
    "\n",
    "**Current Status:** Agent stuck at -21.00 reward (random play) after 1M steps - **NOT LEARNING**\n",
    "\n",
    "**Issues Identified:**\n",
    "1. âŒ Epsilon decays too fast (reaches 0.01 by step 10k, then stays there)\n",
    "2. âŒ No optimistic initialization - agent doesn't explore enough initially\n",
    "3. âŒ Q-values might be converging to bad local minimum\n",
    "4. âŒ Loss is very small but rewards don't improve (overfitting to bad policy)\n",
    "5. âŒ No Double DQN (helps with overestimation bias)\n",
    "\n",
    "**Solutions:**\n",
    "- âœ… Optimistic initialization (initialize Q-values high to encourage exploration)\n",
    "- âœ… Slower epsilon decay (explore longer)\n",
    "- âœ… Double DQN (reduce overestimation)\n",
    "- âœ… Better learning rate schedule\n",
    "- âœ… Reward clipping/normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "474bf381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Improved DQN Agent with optimistic initialization and Double DQN ready!\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED DQN Agent with Optimistic Initialization and Double DQN\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "\n",
    "class ImprovedDQNAgent:\n",
    "    \"\"\"\n",
    "    Improved DQN Agent with:\n",
    "    - Optimistic initialization (early exploration, late exploitation)\n",
    "    - Double DQN (reduces overestimation bias)\n",
    "    - Better epsilon decay schedule\n",
    "    - Learning rate scheduling\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape,\n",
    "        n_actions,\n",
    "        device='cuda',\n",
    "        lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=500000,  # MUCH slower decay - explore for longer\n",
    "        target_update_freq=1000,\n",
    "        buffer_size=100000,\n",
    "        batch_size=32,\n",
    "        optimistic_init=10.0  # Initialize Q-values optimistically\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimistic_init: Initial Q-value (high = more exploration early on)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        self.optimistic_init = optimistic_init\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        n_frames = state_shape[0]\n",
    "        self.q_network = Model(n_actions, n_frames).to(device)\n",
    "        self.target_network = Model(n_actions, n_frames).to(device)\n",
    "        \n",
    "        # OPTIMISTIC INITIALIZATION: Initialize final layer to output high Q-values\n",
    "        # This encourages exploration early on\n",
    "        with torch.no_grad():\n",
    "            self.q_network.fc2.weight.data.fill_(0.0)\n",
    "            self.q_network.fc2.bias.data.fill_(optimistic_init)\n",
    "            self.target_network.fc2.weight.data.fill_(0.0)\n",
    "            self.target_network.fc2.bias.data.fill_(optimistic_init)\n",
    "        \n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Optimizer with learning rate scheduling\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=200000, gamma=0.5)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"Calculate current epsilon - slower decay.\"\"\"\n",
    "        if self.steps < self.epsilon_decay:\n",
    "            return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                   (1 - self.steps / self.epsilon_decay)\n",
    "        else:\n",
    "            return self.epsilon_end\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.get_epsilon():\n",
    "            return random.randrange(self.n_actions)\n",
    "        \n",
    "        # Convert state to tensor and add batch dimension\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action = q_values.argmax(1).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the replay buffer.\"\"\"\n",
    "        # Clip rewards to [-1, 1] for stability\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_tensor = torch.FloatTensor(state).to(self.device) / 255.0\n",
    "        next_state_tensor = torch.FloatTensor(next_state).to(self.device) / 255.0\n",
    "        \n",
    "        self.replay_buffer.push(state_tensor, action, reward, next_state_tensor, done)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step with Double DQN.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Move to device\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Normalize states\n",
    "        states = states / 255.0\n",
    "        next_states = next_states / 255.0\n",
    "        \n",
    "        # Compute Q(s, a)\n",
    "        q_values = self.q_network(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # DOUBLE DQN: Use main network to select action, target network to evaluate\n",
    "        with torch.no_grad():\n",
    "            # Select best action using main network\n",
    "            next_q_values_main = self.q_network(next_states)\n",
    "            next_actions = next_q_values_main.argmax(1)\n",
    "            \n",
    "            # Evaluate using target network\n",
    "            next_q_values_target = self.target_network(next_states)\n",
    "            next_q_value = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_value, target_q_value)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'steps': self.steps,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        if 'scheduler' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        self.steps = checkpoint['steps']\n",
    "\n",
    "print(\"âœ… Improved DQN Agent with optimistic initialization and Double DQN ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b4ba231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 84, 84)\n",
      "Number of actions: 6\n",
      "Using device: cuda\n",
      "ðŸš€ Starting fresh training with improved agent\n",
      "   - Optimistic initialization (explores more early)\n",
      "   - Double DQN (reduces overestimation)\n",
      "   - Slower epsilon decay (explores for 500k steps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1399/2000000 [00:05<5:11:59, 106.76it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4, 210, 160, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     66\u001b[39m pbar = tqdm(\u001b[38;5;28mrange\u001b[39m(total_steps), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, initial=agent.steps \u001b[38;5;28;01mif\u001b[39;00m agent.steps < total_steps \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(agent.steps, total_steps):\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     action = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Take step\u001b[39;00m\n\u001b[32m     73\u001b[39m     next_state, reward, terminated, truncated, info = env.step(action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mImprovedDQNAgent.select_action\u001b[39m\u001b[34m(self, state, training)\u001b[39m\n\u001b[32m     81\u001b[39m state_tensor = torch.FloatTensor(state).unsqueeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device) / \u001b[32m255.0\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     action = q_values.argmax(\u001b[32m1\u001b[39m).item()\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03mForward pass through the network.\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[33;03m    Q-values for each action, shape (batch_size, n_actions)\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Apply convolutional layers with ReLU activation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     42\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n\u001b[32m     43\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\College\\NeuralNetworks\\DQNAtari\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 4, 210, 160, 3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1436/2000000 [00:21<5:11:59, 106.76it/s]"
     ]
    }
   ],
   "source": [
    "# Training with Improved Agent\n",
    "import ale_py\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Create environment with preprocessing\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "env = PreprocessAtari(env)\n",
    "env = FrameStack(env, n_frames=4)\n",
    "\n",
    "# Get environment info\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create IMPROVED agent\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "agent = ImprovedDQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    n_actions=n_actions,\n",
    "    device=device,\n",
    "    lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=500000,  # Explore for 500k steps (was 10k!)\n",
    "    target_update_freq=1000,\n",
    "    buffer_size=100000,\n",
    "    batch_size=32,\n",
    "    optimistic_init=10.0  # High initial Q-values = more exploration\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "total_steps = 2_000_000  # Train longer\n",
    "learning_starts = 10_000\n",
    "train_freq = 4\n",
    "save_freq = 200_000\n",
    "eval_freq = 50_000\n",
    "\n",
    "# Statistics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "current_episode_reward = 0\n",
    "current_episode_length = 0\n",
    "\n",
    "# Load existing model if available\n",
    "model_path = \"models/dqn_pong_improved.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    agent.load(model_path)\n",
    "    print(f\"âœ… Loaded existing model from {model_path}\")\n",
    "    print(f\"   Continuing from step {agent.steps}\")\n",
    "else:\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    print(\"ðŸš€ Starting fresh training with improved agent\")\n",
    "    print(\"   - Optimistic initialization (explores more early)\")\n",
    "    print(\"   - Double DQN (reduces overestimation)\")\n",
    "    print(\"   - Slower epsilon decay (explores for 500k steps)\")\n",
    "\n",
    "# Training loop\n",
    "state, info = env.reset()\n",
    "pbar = tqdm(range(total_steps), desc=\"Training\", initial=agent.steps if agent.steps < total_steps else 0)\n",
    "\n",
    "for step in range(agent.steps, total_steps):\n",
    "    # Select action\n",
    "    action = agent.select_action(state, training=True)\n",
    "    \n",
    "    # Take step\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Store transition\n",
    "    agent.store_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Update statistics\n",
    "    current_episode_reward += reward\n",
    "    current_episode_length += 1\n",
    "    \n",
    "    # Train\n",
    "    if step >= learning_starts and step % train_freq == 0:\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'epsilon': f'{agent.get_epsilon():.3f}',\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'lr': f'{current_lr:.2e}',\n",
    "                'avg_reward': f'{np.mean(episode_rewards[-10:]):.1f}' if len(episode_rewards) >= 10 else 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Handle episode end\n",
    "    if done:\n",
    "        episode_rewards.append(current_episode_reward)\n",
    "        episode_lengths.append(current_episode_length)\n",
    "        current_episode_reward = 0\n",
    "        current_episode_length = 0\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    # Save model periodically\n",
    "    if step > 0 and step % save_freq == 0:\n",
    "        agent.save(model_path)\n",
    "        print(f\"\\nðŸ’¾ Model saved at step {step:,}\")\n",
    "    \n",
    "    # Evaluate and show progress\n",
    "    if step > 0 and step % eval_freq == 0 and len(episode_rewards) >= 10:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_length = np.mean(episode_lengths[-10:])\n",
    "        print(f\"\\nðŸ“Š Step {step:,}: Avg reward (last 10): {avg_reward:.2f}, Avg length: {avg_length:.1f}, Epsilon: {agent.get_epsilon():.3f}\")\n",
    "        \n",
    "        # Show learning progress\n",
    "        if avg_reward > 0:\n",
    "            print(\"   ðŸŽ‰ðŸŽ‰ðŸŽ‰ BREAKTHROUGH! Agent is winning!\")\n",
    "        elif avg_reward > -10:\n",
    "            print(\"   ðŸŽ¯ Great progress! Agent is learning!\")\n",
    "        elif avg_reward > -15:\n",
    "            print(\"   ðŸ“ˆ Starting to improve!\")\n",
    "        elif avg_reward > -19:\n",
    "            print(\"   ðŸ“Š Better than random, keep going!\")\n",
    "        else:\n",
    "            print(\"   â³ Still exploring...\")\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "# Final save\n",
    "agent.save(model_path)\n",
    "env.close()\n",
    "pbar.close()\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Model saved to {model_path}\")\n",
    "print(f\"Total episodes: {len(episode_rewards)}\")\n",
    "if len(episode_rewards) > 0:\n",
    "    print(f\"Final average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "    print(f\"Best average reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "    \n",
    "    # Final assessment\n",
    "    final_avg = np.mean(episode_rewards[-10:])\n",
    "    if final_avg > 10:\n",
    "        print(\"ðŸŒŸðŸŒŸ EXCELLENT! Agent mastered the game!\")\n",
    "    elif final_avg > 0:\n",
    "        print(\"ðŸŽ¯ SUCCESS! Agent is winning more than losing!\")\n",
    "    elif final_avg > -10:\n",
    "        print(\"ðŸ“ˆ Good progress! Agent is learning!\")\n",
    "    elif final_avg > -15:\n",
    "        print(\"ðŸ“Š Some improvement, but needs more training\")\n",
    "    else:\n",
    "        print(\"â³ Still needs work - may need hyperparameter tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4c78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_shape,\n",
    "        n_actions,\n",
    "        device='cuda',\n",
    "        lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=10000,\n",
    "        target_update_freq=1000,\n",
    "        buffer_size=100000,\n",
    "        batch_size=32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_shape: Shape of state (n_frames, height, width)\n",
    "            n_actions: Number of possible actions\n",
    "            device: Device to run on ('cuda' or 'cpu')\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon_start: Starting epsilon for epsilon-greedy\n",
    "            epsilon_end: Final epsilon\n",
    "            epsilon_decay: Steps to decay epsilon\n",
    "            target_update_freq: Frequency to update target network\n",
    "            buffer_size: Size of replay buffer\n",
    "            batch_size: Batch size for training\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Create Q-network and target network\n",
    "        n_frames = state_shape[0]\n",
    "        self.q_network = Model(n_actions, n_frames).to(device)\n",
    "        self.target_network = Model(n_actions, n_frames).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()  # Target network is always in eval mode\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"Calculate current epsilon for epsilon-greedy policy.\"\"\"\n",
    "        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "               math.exp(-1. * self.steps / self.epsilon_decay)\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (n_frames, height, width)\n",
    "            training: If True, use epsilon-greedy; if False, use greedy\n",
    "        \n",
    "        Returns:\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.get_epsilon():\n",
    "            return random.randrange(self.n_actions)\n",
    "        \n",
    "        # Convert state to tensor and add batch dimension\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action = q_values.argmax(1).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition in the replay buffer.\"\"\"\n",
    "        # Convert to tensors\n",
    "        state_tensor = torch.FloatTensor(state).to(self.device) / 255.0\n",
    "        next_state_tensor = torch.FloatTensor(next_state).to(self.device) / 255.0\n",
    "        \n",
    "        self.replay_buffer.push(state_tensor, action, reward, next_state_tensor, done)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Move to device\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        \n",
    "        # Normalize states\n",
    "        states = states / 255.0\n",
    "        next_states = next_states / 255.0\n",
    "        \n",
    "        # Compute Q(s, a)\n",
    "        q_values = self.q_network(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            next_q_value = next_q_values.max(1)[0]\n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_value, target_q_value)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load the model.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps = checkpoint['steps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31562234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All classes and imports are ready!\n",
      "âœ… You can now run the training cell to start training!\n"
     ]
    }
   ],
   "source": [
    "# Quick verification - Run this before training to make sure everything is set up\n",
    "try:\n",
    "    # Check if all classes are defined\n",
    "    assert 'Model' in globals(), \"Model class not found - run the Model cell first!\"\n",
    "    assert 'ReplayBuffer' in globals(), \"ReplayBuffer class not found - run the ReplayBuffer cell first!\"\n",
    "    assert 'PreprocessAtari' in globals(), \"PreprocessAtari class not found - run the preprocessing cell first!\"\n",
    "    assert 'FrameStack' in globals(), \"FrameStack class not found - run the preprocessing cell first!\"\n",
    "    assert 'DQNAgent' in globals(), \"DQNAgent class not found - run the DQNAgent cell first!\"\n",
    "    \n",
    "    # Check imports\n",
    "    import torch\n",
    "    import gymnasium as gym\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(\"âœ… All classes and imports are ready!\")\n",
    "    print(\"âœ… You can now run the training cell to start training!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ {e}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Missing import: {e}\")\n",
    "    print(\"Make sure you've run all the setup cells in order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa8c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 84, 84)\n",
      "Number of actions: 6\n",
      "Using device: cuda\n",
      "Loaded model from models/dqn_pong.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 10045/1000000 [00:11<20:35, 801.30it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–         | 20053/1000000 [00:34<40:23, 404.42it/s, epsilon=0.010, loss=0.0601, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 20000: Avg reward (last 10): -20.80, Avg length: 791.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–Ž         | 30050/1000000 [00:58<38:42, 417.72it/s, epsilon=0.010, loss=0.0595, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 30000: Avg reward (last 10): -21.00, Avg length: 819.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–         | 40102/1000000 [01:17<28:33, 560.05it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 40000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–Œ         | 50056/1000000 [01:34<27:46, 570.10it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 50000: Avg reward (last 10): -20.90, Avg length: 826.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–Œ         | 60097/1000000 [01:52<27:36, 567.27it/s, epsilon=0.010, loss=0.0301, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 60000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|â–‹         | 70043/1000000 [02:10<27:05, 572.16it/s, epsilon=0.010, loss=0.0597, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 70000: Avg reward (last 10): -20.90, Avg length: 785.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–Š         | 80037/1000000 [02:29<35:57, 426.49it/s, epsilon=0.010, loss=0.0005, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 80000: Avg reward (last 10): -20.90, Avg length: 783.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–‰         | 90037/1000000 [02:53<35:46, 423.91it/s, epsilon=0.010, loss=0.0596, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 90000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆ         | 100064/1000000 [03:16<34:40, 432.54it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 100000\n",
      "\n",
      "Step 100000: Avg reward (last 10): -21.00, Avg length: 794.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆ         | 110078/1000000 [03:37<30:44, 482.52it/s, epsilon=0.010, loss=0.0598, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 110000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|â–ˆâ–        | 120077/1000000 [03:58<30:01, 488.39it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 120000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–Ž        | 130070/1000000 [04:19<30:10, 480.56it/s, epsilon=0.010, loss=0.0005, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 130000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–        | 140073/1000000 [04:40<29:58, 478.08it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 140000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–Œ        | 150039/1000000 [05:01<30:23, 466.17it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 150000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–Œ        | 160087/1000000 [05:22<29:07, 480.61it/s, epsilon=0.010, loss=0.0009, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 160000: Avg reward (last 10): -21.00, Avg length: 772.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|â–ˆâ–‹        | 170080/1000000 [05:43<28:35, 483.75it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 170000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–Š        | 180055/1000000 [06:04<28:33, 478.44it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 180000: Avg reward (last 10): -21.00, Avg length: 783.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–‰        | 190062/1000000 [06:25<28:13, 478.13it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 190000: Avg reward (last 10): -21.00, Avg length: 781.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆ        | 200088/1000000 [06:46<29:04, 458.57it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 200000\n",
      "\n",
      "Step 200000: Avg reward (last 10): -21.00, Avg length: 788.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆ        | 210037/1000000 [07:08<31:02, 424.25it/s, epsilon=0.010, loss=0.0597, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 210000: Avg reward (last 10): -21.00, Avg length: 788.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|â–ˆâ–ˆâ–       | 220061/1000000 [07:31<29:45, 436.87it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 220000: Avg reward (last 10): -21.00, Avg length: 776.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–Ž       | 230037/1000000 [07:54<29:28, 435.37it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 230000: Avg reward (last 10): -20.90, Avg length: 801.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–       | 240061/1000000 [08:17<29:21, 431.31it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 240000: Avg reward (last 10): -21.00, Avg length: 791.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 250065/1000000 [08:40<28:32, 437.99it/s, epsilon=0.010, loss=0.0595, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 250000: Avg reward (last 10): -20.90, Avg length: 786.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 260058/1000000 [09:03<27:45, 444.34it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 260000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|â–ˆâ–ˆâ–‹       | 270065/1000000 [09:26<27:59, 434.56it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 270000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–Š       | 280053/1000000 [09:49<27:34, 435.20it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 280000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–‰       | 290037/1000000 [10:12<27:15, 434.08it/s, epsilon=0.010, loss=0.0598, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 290000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆ       | 300053/1000000 [10:35<27:30, 424.10it/s, epsilon=0.010, loss=0.0301, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 300000\n",
      "\n",
      "Step 300000: Avg reward (last 10): -21.00, Avg length: 774.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 310040/1000000 [10:57<26:25, 435.23it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 310000: Avg reward (last 10): -20.90, Avg length: 785.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 320059/1000000 [11:20<26:10, 433.06it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 320000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 330051/1000000 [11:43<25:13, 442.64it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 330000: Avg reward (last 10): -21.00, Avg length: 770.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 340059/1000000 [12:06<25:08, 437.36it/s, epsilon=0.010, loss=0.0305, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 340000: Avg reward (last 10): -20.90, Avg length: 789.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 350044/1000000 [12:29<24:27, 442.93it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 350000: Avg reward (last 10): -20.90, Avg length: 777.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 360066/1000000 [12:52<24:15, 439.81it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 360000: Avg reward (last 10): -21.00, Avg length: 770.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 370037/1000000 [13:14<23:53, 439.34it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 370000: Avg reward (last 10): -21.00, Avg length: 772.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 380070/1000000 [13:37<23:30, 439.56it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 380000: Avg reward (last 10): -21.00, Avg length: 778.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 390053/1000000 [14:00<23:21, 435.25it/s, epsilon=0.010, loss=0.0333, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 390000: Avg reward (last 10): -21.00, Avg length: 784.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 400036/1000000 [14:23<29:12, 342.43it/s, epsilon=0.010, loss=0.0305, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 400000\n",
      "\n",
      "Step 400000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 410056/1000000 [14:46<22:21, 439.81it/s, epsilon=0.010, loss=0.0602, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 410000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 420054/1000000 [15:09<22:03, 438.24it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 420000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 430053/1000000 [15:32<21:51, 434.53it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 430000: Avg reward (last 10): -21.00, Avg length: 782.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440038/1000000 [15:55<21:31, 433.64it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 440000: Avg reward (last 10): -21.00, Avg length: 798.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 450078/1000000 [16:18<20:55, 438.15it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 450000: Avg reward (last 10): -20.90, Avg length: 794.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 460041/1000000 [16:41<20:53, 430.92it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 460000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 470053/1000000 [17:04<19:02, 463.91it/s, epsilon=0.010, loss=0.0598, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 470000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 480075/1000000 [17:26<19:23, 446.78it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 480000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 490043/1000000 [17:48<18:38, 456.08it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 490000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500051/1000000 [18:10<19:07, 435.56it/s, epsilon=0.010, loss=0.0306, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 500000\n",
      "\n",
      "Step 500000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 510056/1000000 [18:32<17:46, 459.31it/s, epsilon=0.010, loss=0.0598, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 510000: Avg reward (last 10): -21.00, Avg length: 769.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 520069/1000000 [18:54<17:46, 449.82it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 520000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 530073/1000000 [19:16<17:16, 453.26it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 530000: Avg reward (last 10): -21.00, Avg length: 777.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 540041/1000000 [19:38<17:24, 440.45it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 540000: Avg reward (last 10): -21.00, Avg length: 842.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 550081/1000000 [20:01<16:42, 448.92it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 550000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 560084/1000000 [20:23<16:00, 457.99it/s, epsilon=0.010, loss=0.0600, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 560000: Avg reward (last 10): -21.00, Avg length: 812.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 570059/1000000 [20:45<15:49, 452.99it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 570000: Avg reward (last 10): -21.00, Avg length: 776.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 580061/1000000 [21:07<15:23, 454.81it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 580000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 590069/1000000 [21:29<15:07, 451.52it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 590000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 600038/1000000 [21:51<15:33, 428.38it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 600000\n",
      "\n",
      "Step 600000: Avg reward (last 10): -21.00, Avg length: 766.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 610083/1000000 [22:13<14:18, 454.36it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 610000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 620061/1000000 [22:35<14:07, 448.45it/s, epsilon=0.010, loss=0.0626, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 620000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 630048/1000000 [22:57<13:38, 451.74it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 630000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 640049/1000000 [23:19<13:08, 456.69it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 640000: Avg reward (last 10): -20.90, Avg length: 791.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 650059/1000000 [23:41<12:49, 454.74it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 650000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 660065/1000000 [24:04<13:31, 418.95it/s, epsilon=0.010, loss=0.0305, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 660000: Avg reward (last 10): -20.90, Avg length: 772.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 670051/1000000 [24:27<12:36, 435.92it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 670000: Avg reward (last 10): -21.00, Avg length: 793.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 680043/1000000 [24:50<12:08, 439.11it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 680000: Avg reward (last 10): -21.00, Avg length: 812.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 690060/1000000 [25:13<11:47, 438.27it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 690000: Avg reward (last 10): -21.00, Avg length: 794.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 700049/1000000 [25:36<11:53, 420.52it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 700000\n",
      "\n",
      "Step 700000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 710078/1000000 [25:59<11:05, 435.40it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 710000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 720057/1000000 [26:22<10:39, 437.89it/s, epsilon=0.010, loss=0.0627, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 720000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 730044/1000000 [26:45<10:36, 424.36it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 730000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 740038/1000000 [27:08<09:48, 441.53it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 740000: Avg reward (last 10): -21.00, Avg length: 775.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 750036/1000000 [27:31<09:50, 423.28it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 750000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 760078/1000000 [27:54<09:07, 438.39it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 760000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 770063/1000000 [28:17<08:50, 433.16it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 770000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 780065/1000000 [28:40<08:24, 436.33it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 780000: Avg reward (last 10): -21.00, Avg length: 768.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 790048/1000000 [29:03<07:52, 444.06it/s, epsilon=0.010, loss=0.0600, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 790000: Avg reward (last 10): -21.00, Avg length: 768.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 800039/1000000 [29:26<08:02, 414.77it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 800000\n",
      "\n",
      "Step 800000: Avg reward (last 10): -20.90, Avg length: 791.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 810068/1000000 [29:49<07:17, 434.12it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 810000: Avg reward (last 10): -21.00, Avg length: 795.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 820062/1000000 [30:12<06:54, 434.02it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 820000: Avg reward (last 10): -21.00, Avg length: 772.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 830054/1000000 [30:35<06:37, 427.33it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 830000: Avg reward (last 10): -21.00, Avg length: 788.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 840060/1000000 [30:58<06:17, 423.19it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 840000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 850069/1000000 [31:21<05:42, 438.37it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 850000: Avg reward (last 10): -21.00, Avg length: 791.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 860036/1000000 [31:44<05:26, 428.79it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 860000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 870060/1000000 [32:07<04:56, 438.53it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 870000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 880079/1000000 [32:30<04:31, 441.62it/s, epsilon=0.010, loss=0.0302, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 880000: Avg reward (last 10): -21.00, Avg length: 806.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 890053/1000000 [32:53<04:17, 427.32it/s, epsilon=0.010, loss=0.0008, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 890000: Avg reward (last 10): -21.00, Avg length: 766.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 900060/1000000 [33:16<03:56, 422.11it/s, epsilon=0.010, loss=0.0599, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved at step 900000\n",
      "\n",
      "Step 900000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 910057/1000000 [33:39<03:31, 424.30it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 910000: Avg reward (last 10): -20.60, Avg length: 850.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 920046/1000000 [34:02<03:03, 436.61it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 920000: Avg reward (last 10): -21.00, Avg length: 854.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 930075/1000000 [34:25<02:40, 434.33it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 930000: Avg reward (last 10): -21.00, Avg length: 782.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 940074/1000000 [34:48<02:15, 441.91it/s, epsilon=0.010, loss=0.0006, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 940000: Avg reward (last 10): -20.90, Avg length: 815.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 950037/1000000 [35:11<01:54, 436.78it/s, epsilon=0.010, loss=0.0305, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 950000: Avg reward (last 10): -21.00, Avg length: 764.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 960067/1000000 [35:34<01:31, 438.01it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 960000: Avg reward (last 10): -20.90, Avg length: 852.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 970067/1000000 [35:57<01:08, 434.09it/s, epsilon=0.010, loss=0.0007, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 970000: Avg reward (last 10): -21.00, Avg length: 830.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 980047/1000000 [36:20<00:46, 432.33it/s, epsilon=0.010, loss=0.0304, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 980000: Avg reward (last 10): -21.00, Avg length: 770.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 990041/1000000 [36:43<00:23, 422.61it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 990000: Avg reward (last 10): -21.00, Avg length: 776.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000000/1000000 [37:06<00:00, 449.13it/s, epsilon=0.010, loss=0.0303, ep_reward=N/A] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete! Model saved to models/dqn_pong.pth\n",
      "Total episodes: 1276\n",
      "Final average reward (last 100): -20.98\n"
     ]
    }
   ],
   "source": [
    "import ale_py\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create environment with preprocessing\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "env = PreprocessAtari(env)\n",
    "env = FrameStack(env, n_frames=4)\n",
    "\n",
    "# Get environment info\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Create agent\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    n_actions=n_actions,\n",
    "    device=device,\n",
    "    lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=10000,\n",
    "    target_update_freq=1000,\n",
    "    buffer_size=100000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "total_steps = 1_000_000\n",
    "learning_starts = 10_000  # Start training after this many steps\n",
    "train_freq = 4  # Train every N steps\n",
    "save_freq = 100_000  # Save model every N steps\n",
    "eval_freq = 10_000  # Evaluate every N steps\n",
    "\n",
    "# Statistics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "current_episode_reward = 0\n",
    "current_episode_length = 0\n",
    "\n",
    "# Load existing model if available\n",
    "model_path = \"models/dqn_pong.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    agent.load(model_path)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "else:\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "# Training loop\n",
    "state, info = env.reset()\n",
    "pbar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "    # Select action\n",
    "    action = agent.select_action(state, training=True)\n",
    "    \n",
    "    # Take step\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Store transition\n",
    "    agent.store_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Update statistics\n",
    "    current_episode_reward += reward\n",
    "    current_episode_length += 1\n",
    "    \n",
    "    # Train\n",
    "    if step >= learning_starts and step % train_freq == 0:\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            pbar.set_postfix({\n",
    "                'epsilon': f'{agent.get_epsilon():.3f}',\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'ep_reward': f'{current_episode_reward:.1f}' if done else 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Handle episode end\n",
    "    if done:\n",
    "        episode_rewards.append(current_episode_reward)\n",
    "        episode_lengths.append(current_episode_length)\n",
    "        current_episode_reward = 0\n",
    "        current_episode_length = 0\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    # Save model\n",
    "    if step > 0 and step % save_freq == 0:\n",
    "        agent.save(model_path)\n",
    "        print(f\"\\nModel saved at step {step}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    if step > 0 and step % eval_freq == 0 and len(episode_rewards) > 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "        avg_length = np.mean(episode_lengths[-10:]) if len(episode_lengths) >= 10 else np.mean(episode_lengths)\n",
    "        print(f\"\\nStep {step}: Avg reward (last 10): {avg_reward:.2f}, Avg length: {avg_length:.1f}\")\n",
    "\n",
    "# Final save\n",
    "agent.save(model_path)\n",
    "env.close()\n",
    "print(f\"\\nTraining complete! Model saved to {model_path}\")\n",
    "print(f\"Total episodes: {len(episode_rewards)}\")\n",
    "if len(episode_rewards) > 0:\n",
    "    print(f\"Final average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc633f8c",
   "metadata": {},
   "source": [
    "## Training Analysis & Answers to Your Questions\n",
    "\n",
    "**Has it learned anything?**\n",
    "Looking at your training output:\n",
    "- Average reward: **-20.40 to -21.00** (basically random play - losing every point)\n",
    "- For Pong, rewards range from -21 (lose all points) to +21 (win all points)\n",
    "- **The agent hasn't learned much yet** - it's still playing randomly\n",
    "\n",
    "**Should you increase to 1,000,000 steps?**\n",
    "**YES!** DQN typically needs:\n",
    "- **100,000 steps**: Just starting to learn (you're here)\n",
    "- **500,000 steps**: Beginning to show improvement\n",
    "- **1,000,000+ steps**: Actually learning to play well\n",
    "- **10,000,000 steps**: Master-level play\n",
    "\n",
    "**Why does it \"learn so fast\"?**\n",
    "It's not actually learning fast - the **loss is decreasing** (which is good), but the **rewards aren't improving yet**. This is normal! The network is learning the Q-function, but it takes time for that to translate to better gameplay.\n",
    "\n",
    "**Are we using a premade agent?**\n",
    "**NO!** We built everything from scratch:\n",
    "- âœ… Custom Model class (CNN architecture)\n",
    "- âœ… Custom ReplayBuffer (experience replay)\n",
    "- âœ… Custom DQNAgent (with target network, epsilon-greedy, etc.)\n",
    "- âœ… Custom training loop\n",
    "\n",
    "This is a **fully custom implementation** - not using stable-baselines3 or any pre-made agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44192bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model from models/dqn_pong.pth\n",
      "   Model was trained for 100000 steps\n",
      "\n",
      "Running 10 evaluation episodes (no exploration, greedy policy)...\n",
      "Episode 1: Reward = -21.0, Length = 764\n",
      "Episode 2: Reward = -21.0, Length = 764\n",
      "Episode 3: Reward = -21.0, Length = 764\n",
      "Episode 4: Reward = -21.0, Length = 764\n",
      "Episode 5: Reward = -21.0, Length = 764\n",
      "Episode 6: Reward = -21.0, Length = 764\n",
      "Episode 7: Reward = -21.0, Length = 764\n",
      "Episode 8: Reward = -21.0, Length = 764\n",
      "Episode 9: Reward = -21.0, Length = 764\n",
      "Episode 10: Reward = -21.0, Length = 764\n",
      "\n",
      "==================================================\n",
      "Evaluation Summary (10 episodes):\n",
      "  Average Reward: -21.00\n",
      "  Best Reward: -21.00\n",
      "  Worst Reward: -21.00\n",
      "  Average Length: 764.0 steps\n",
      "==================================================\n",
      "ðŸ“‰ Status: Still playing randomly (needs more training)\n",
      "   â†’ Increase training to 1,000,000+ steps\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "import ale_py\n",
    "import numpy as np\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = gym.make(\"ALE/Pong-v5\")\n",
    "eval_env = PreprocessAtari(eval_env)\n",
    "eval_env = FrameStack(eval_env, n_frames=4)\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"models/dqn_pong.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    # Recreate agent with same parameters\n",
    "    state_shape = eval_env.observation_space.shape\n",
    "    n_actions = eval_env.action_space.n\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    eval_agent = DQNAgent(\n",
    "        state_shape=state_shape,\n",
    "        n_actions=n_actions,\n",
    "        device=device,\n",
    "        lr=1e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=10000,\n",
    "        target_update_freq=1000,\n",
    "        buffer_size=100000,\n",
    "        batch_size=32\n",
    "    )\n",
    "    eval_agent.load(model_path)\n",
    "    print(f\"âœ… Loaded model from {model_path}\")\n",
    "    print(f\"   Model was trained for {eval_agent.steps} steps\")\n",
    "else:\n",
    "    print(\"âŒ No model found to evaluate\")\n",
    "    eval_env.close()\n",
    "\n",
    "# Run evaluation episodes\n",
    "n_eval_episodes = 10\n",
    "eval_rewards = []\n",
    "eval_lengths = []\n",
    "\n",
    "print(f\"\\nRunning {n_eval_episodes} evaluation episodes (no exploration, greedy policy)...\")\n",
    "for episode in range(n_eval_episodes):\n",
    "    state, info = eval_env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    while True:\n",
    "        # Use greedy policy (no exploration)\n",
    "        action = eval_agent.select_action(state, training=False)\n",
    "        state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    eval_lengths.append(episode_length)\n",
    "    print(f\"Episode {episode+1}: Reward = {episode_reward:+.1f}, Length = {episode_length}\")\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Evaluation Summary ({n_eval_episodes} episodes):\")\n",
    "print(f\"  Average Reward: {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"  Best Reward: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"  Worst Reward: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"  Average Length: {np.mean(eval_lengths):.1f} steps\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Interpretation\n",
    "avg_reward = np.mean(eval_rewards)\n",
    "if avg_reward < -19:\n",
    "    print(\"ðŸ“‰ Status: Still playing randomly (needs more training)\")\n",
    "    print(\"   â†’ Increase training to 1,000,000+ steps\")\n",
    "elif avg_reward < -10:\n",
    "    print(\"ðŸ“ˆ Status: Starting to learn (showing some improvement)\")\n",
    "    print(\"   â†’ Continue training to see more improvement\")\n",
    "elif avg_reward < 0:\n",
    "    print(\"ðŸŽ¯ Status: Learning! (better than random)\")\n",
    "    print(\"   â†’ Keep training to reach positive rewards\")\n",
    "elif avg_reward < 10:\n",
    "    print(\"ðŸ† Status: Playing well! (winning some games)\")\n",
    "    print(\"   â†’ Excellent progress!\")\n",
    "else:\n",
    "    print(\"ðŸŒŸ Status: Master level! (consistently winning)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6ac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    DQN Model for Atari games.\n",
    "    Takes stacked frames as input and outputs Q-values for each action.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions, n_frames=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Number of possible actions (e.g., 6 for Pong)\n",
    "            n_frames: Number of stacked frames (default: 4)\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Convolutional layers to process the image frames\n",
    "        self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate the size of the flattened feature map\n",
    "        # Input shape: (n_frames, 84, 84) after preprocessing (or 210x160x3 raw)\n",
    "        # After conv layers, we need to calculate the output size\n",
    "        # For standard Atari preprocessing (84x84), the output is 7x7x64\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, n_frames, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Q-values for each action, shape (batch_size, n_actions)\n",
    "        \"\"\"\n",
    "        # Apply convolutional layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten the feature map\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "jupyterlab": {
   "codeCellConfig": {
    "lineNumbers": true
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
