{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fbc121e30a2defb3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-12T17:48:04.941417700Z",
          "start_time": "2025-12-12T17:48:04.061872100Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (25.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.45.1)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U pip setuptools wheel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "a86c69207428aa2b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-12T17:48:09.919701600Z",
          "start_time": "2025-12-12T17:48:06.663746900Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
            "Requirement already satisfied: torch in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1+cu128)\n",
            "Requirement already satisfied: torchvision in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.24.1+cu128)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1+cu128)\n",
            "Requirement already satisfied: filelock in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e11d76180279cb95",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-12T17:51:44.327759500Z",
          "start_time": "2025-12-12T17:51:43.524465Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gymnasium[atari]) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gymnasium[atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gymnasium[atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in c:\\users\\sophi\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gymnasium[atari]) (0.11.2)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install -U \"gymnasium[atari]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8156fcaa8935feb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-12T17:54:29.154560500Z",
          "start_time": "2025-12-12T17:53:59.299457500Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.9.1+cu128\n",
            "cuda available: True\n",
            "gpu: NVIDIA GeForce RTX 3060\n",
            "torch cuda version: 12.8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "    print(\"torch cuda version:\", torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "96b7abb0163fa3a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-12T17:54:53.888932600Z",
          "start_time": "2025-12-12T17:54:52.617816800Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obs shape: (210, 160, 3)\n",
            "ok\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py  # registers ALE envs\n",
        "\n",
        "env = gym.make(\"ALE/Pong-v5\")  # no rendering needed for a smoke test\n",
        "obs, info = env.reset()\n",
        "print(\"obs shape:\", obs.shape)\n",
        "\n",
        "for _ in range(200):\n",
        "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "    if terminated or truncated:\n",
        "        obs, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "print(\"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c6dae33f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ opencv-python is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install opencv-python if not already installed (required for PreprocessAtari wrapper)\n",
        "try:\n",
        "    import cv2\n",
        "    print(\"‚úÖ opencv-python is already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing opencv-python...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python\"])\n",
        "    print(\"‚úÖ opencv-python installed successfully!\")\n",
        "    import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5770f579",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure torch is imported (if not already imported in earlier cells)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "59e1871c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocessing wrappers fixed using ObservationWrapper!\n",
            "   - PreprocessAtari now uses ObservationWrapper for automatic transformation\n",
            "   - FrameStack handles shape validation and dtype conversion\n"
          ]
        }
      ],
      "source": [
        "# FIXED: PreprocessAtari wrapper using ObservationWrapper for proper observation transformation\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import ObservationWrapper, Wrapper\n",
        "import cv2\n",
        "from collections import deque\n",
        "\n",
        "class PreprocessAtari(ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Preprocesses Atari frames: resize to 84x84, convert to grayscale.\n",
        "    Uses ObservationWrapper to ensure observations are automatically transformed.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Update observation space after transformation\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84), dtype=np.uint8\n",
        "        )\n",
        "    \n",
        "    def observation(self, obs):\n",
        "        \"\"\"\n",
        "        Transform observation: RGB -> Grayscale -> Resize to 84x84\n",
        "        This method is automatically called by ObservationWrapper for all observations.\n",
        "        \"\"\"\n",
        "        # Ensure we have the right input shape\n",
        "        if len(obs.shape) == 3 and obs.shape[2] == 3:\n",
        "            # Convert RGB to grayscale\n",
        "            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        elif len(obs.shape) == 3 and obs.shape[2] == 1:\n",
        "            # Already grayscale, just remove channel dimension\n",
        "            obs = obs.squeeze(2)\n",
        "        elif len(obs.shape) == 2:\n",
        "            # Already 2D grayscale\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected observation shape: {obs.shape}\")\n",
        "        \n",
        "        # Resize to 84x84 (handles both grayscale and color inputs)\n",
        "        if obs.shape != (84, 84):\n",
        "            obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        # Ensure output is uint8 and 2D\n",
        "        obs = obs.astype(np.uint8)\n",
        "        if len(obs.shape) == 3:\n",
        "            obs = obs.squeeze()\n",
        "        \n",
        "        return obs\n",
        "\n",
        "class FrameStack(Wrapper):\n",
        "    \"\"\"\n",
        "    Stacks the last n frames together.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_frames=4):\n",
        "        super().__init__(env)\n",
        "        self.n_frames = n_frames\n",
        "        self.frames = deque(maxlen=n_frames)\n",
        "        \n",
        "        # Update observation space\n",
        "        obs_shape = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, \n",
        "            shape=(n_frames, obs_shape[0], obs_shape[1]), \n",
        "            dtype=np.uint8\n",
        "        )\n",
        "    \n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        # Ensure obs is 2D (height, width)\n",
        "        if len(obs.shape) > 2:\n",
        "            obs = obs.squeeze()\n",
        "        # Fill the frame stack with the first frame\n",
        "        for _ in range(self.n_frames):\n",
        "            self.frames.append(obs.copy())\n",
        "        return self._get_obs(), info\n",
        "    \n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        # Ensure obs is 2D (height, width)\n",
        "        if len(obs.shape) > 2:\n",
        "            obs = obs.squeeze()\n",
        "        self.frames.append(obs.copy())\n",
        "        return self._get_obs(), reward, terminated, truncated, info\n",
        "    \n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Stack frames: (n_frames, height, width)\n",
        "        Returns numpy array of shape (n_frames, H, W)\n",
        "        \"\"\"\n",
        "        # Ensure all frames are 2D\n",
        "        frames_2d = []\n",
        "        for frame in self.frames:\n",
        "            if len(frame.shape) > 2:\n",
        "                frame = frame.squeeze()\n",
        "            frames_2d.append(frame)\n",
        "        \n",
        "        # Stack along first dimension: (n_frames, height, width)\n",
        "        stacked = np.stack(frames_2d, axis=0)\n",
        "        \n",
        "        # Ensure dtype is uint8\n",
        "        stacked = stacked.astype(np.uint8)\n",
        "        \n",
        "        return stacked\n",
        "\n",
        "class FireReset(gym.Wrapper):\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        obs, _, terminated, truncated, info = self.env.step(1)  # FIRE\n",
        "        if terminated or truncated:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "class ReducedActionSet(gym.ActionWrapper):\n",
        "    def __init__(self, env, allowed_actions):\n",
        "        super().__init__(env)\n",
        "        self.allowed_actions = allowed_actions\n",
        "        self.action_space = gym.spaces.Discrete(len(allowed_actions))\n",
        "\n",
        "    def action(self, a):\n",
        "        return self.allowed_actions[a]\n",
        "\n",
        "\n",
        "\n",
        "print(\"‚úÖ Preprocessing wrappers fixed using ObservationWrapper!\")\n",
        "print(\"   - PreprocessAtari now uses ObservationWrapper for automatic transformation\")\n",
        "print(\"   - FrameStack handles shape validation and dtype conversion\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7835796e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # Store raw uint8 frames on CPU (compact, fast)\n",
        "        self.buffer.append(Experience(\n",
        "            state.astype(np.uint8),\n",
        "            int(action),\n",
        "            float(reward),\n",
        "            next_state.astype(np.uint8),\n",
        "            bool(done),\n",
        "        ))\n",
        "\n",
        "    def sample(self, batch_size: int, device: str):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        states = np.stack([e.state for e in batch])          # (B, 4, 84, 84) uint8\n",
        "        next_states = np.stack([e.next_state for e in batch])# (B, 4, 84, 84) uint8\n",
        "\n",
        "        # Convert to float + normalize ONCE here\n",
        "        states = torch.as_tensor(states, device=device, dtype=torch.float32) / 255.0\n",
        "        next_states = torch.as_tensor(next_states, device=device, dtype=torch.float32) / 255.0\n",
        "\n",
        "        actions = torch.as_tensor([e.action for e in batch], device=device, dtype=torch.long)\n",
        "        rewards = torch.as_tensor([e.reward for e in batch], device=device, dtype=torch.float32)\n",
        "        dones = torch.as_tensor([e.done for e in batch], device=device, dtype=torch.float32)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "84c1fe3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DIAGNOSTIC CHECK ===\n",
            "\n",
            "1. Checking observation preprocessing...\n",
            "   ‚úÖ Observation shape: (4, 84, 84) (should be (4, 84, 84))\n",
            "   ‚úÖ Observation dtype: uint8 (should be uint8)\n",
            "\n",
            "2. Checking saved model...\n",
            "   ‚úÖ Model exists, trained for 1200001 steps\n",
            "\n",
            "3. Checking Q-network outputs...\n",
            "   ‚ùå ERROR: Model class not found! Make sure you've run the Model cell.\n",
            "   ‚ùå Error checking model: Model class not found in global namespace\n",
            "\n",
            "4. Common issues checklist:\n",
            "   ‚ñ° Wrapper preprocessing observations correctly\n",
            "   ‚ñ° Learning rate not too small (should be ~1e-4)\n",
            "   ‚ñ° Batch size reasonable (32 is good)\n",
            "   ‚ñ° Replay buffer filling up (needs >10k samples)\n",
            "   ‚ñ° Training actually happening (check loss values)\n",
            "   ‚ñ° Epsilon decay schedule appropriate\n",
            "\n",
            "=== RECOMMENDATIONS ===\n",
            "If rewards are stuck at -21 (random play):\n",
            "1. Verify the wrapper cell has been re-run after fixes\n",
            "2. Check if loss is actually changing during training\n",
            "3. Try increasing learning rate to 2e-4 or 5e-4\n",
            "4. Verify Double DQN is working (check train_step method)\n",
            "5. Consider resetting and starting fresh with improved agent\n"
          ]
        }
      ],
      "source": [
        "# üîç DIAGNOSTIC: Check why agent isn't learning at 1.3M steps\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"=== DIAGNOSTIC CHECK ===\")\n",
        "\n",
        "# Check 1: Verify wrapper is working\n",
        "print(\"\\n1. Checking observation preprocessing...\")\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    import ale_py\n",
        "    test_env = gym.make(\"ALE/Pong-v5\")\n",
        "    test_env = PreprocessAtari(test_env)\n",
        "    test_env = FrameStack(test_env, n_frames=4)\n",
        "    test_obs, _ = test_env.reset()\n",
        "    print(f\"   ‚úÖ Observation shape: {test_obs.shape} (should be (4, 84, 84))\")\n",
        "    print(f\"   ‚úÖ Observation dtype: {test_obs.dtype} (should be uint8)\")\n",
        "    if test_obs.shape != (4, 84, 84):\n",
        "        print(f\"   ‚ùå ERROR: Shape mismatch! This is why it's not learning!\")\n",
        "        raise ValueError(\"Wrapper not working!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå ERROR in wrapper: {e}\")\n",
        "\n",
        "# Check 2: Check current model metrics\n",
        "print(\"\\n2. Checking saved model...\")\n",
        "model_path = \"models/dqn_pong_improved.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    print(f\"   ‚úÖ Model exists, trained for {checkpoint.get('steps', 'unknown')} steps\")\n",
        "    \n",
        "    # Check Q-values to see if they're reasonable\n",
        "    print(\"\\n3. Checking Q-network outputs...\")\n",
        "    try:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        state_shape = (4, 84, 84)\n",
        "        n_actions = 6\n",
        "        n_frames = state_shape[0]\n",
        "        \n",
        "        # Create a dummy model to check structure\n",
        "        # Model should be defined in a previous cell\n",
        "        if 'Model' not in globals():\n",
        "            print(f\"   ‚ùå ERROR: Model class not found! Make sure you've run the Model cell.\")\n",
        "            raise NameError(\"Model class not found in global namespace\")\n",
        "        test_model = Model(n_actions, n_frames).to(device)\n",
        "        test_model.load_state_dict(checkpoint['q_network'])\n",
        "        test_model.eval()\n",
        "        \n",
        "        # Test with dummy state\n",
        "        dummy_state = torch.zeros(1, 4, 84, 84).to(device) / 255.0\n",
        "        with torch.no_grad():\n",
        "            q_values = test_model(dummy_state)\n",
        "        \n",
        "        print(f\"   ‚úÖ Q-values shape: {q_values.shape}\")\n",
        "        print(f\"   ‚úÖ Q-values range: [{q_values.min().item():.2f}, {q_values.max().item():.2f}]\")\n",
        "        print(f\"   ‚úÖ Q-values mean: {q_values.mean().item():.2f}\")\n",
        "        \n",
        "        if q_values.abs().max() < 0.1:\n",
        "            print(f\"   ‚ö†Ô∏è  WARNING: Q-values are very small! Network might not be learning.\")\n",
        "        if q_values.max() - q_values.min() < 0.01:\n",
        "            print(f\"   ‚ö†Ô∏è  WARNING: Q-values are almost identical! No action differentiation.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error checking model: {e}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Model file not found at {model_path}\")\n",
        "\n",
        "# Check 4: Common issues\n",
        "print(\"\\n4. Common issues checklist:\")\n",
        "print(\"   ‚ñ° Wrapper preprocessing observations correctly\")\n",
        "print(\"   ‚ñ° Learning rate not too small (should be ~1e-4)\")\n",
        "print(\"   ‚ñ° Batch size reasonable (32 is good)\")\n",
        "print(\"   ‚ñ° Replay buffer filling up (needs >10k samples)\")\n",
        "print(\"   ‚ñ° Training actually happening (check loss values)\")\n",
        "print(\"   ‚ñ° Epsilon decay schedule appropriate\")\n",
        "\n",
        "print(\"\\n=== RECOMMENDATIONS ===\")\n",
        "print(\"If rewards are stuck at -21 (random play):\")\n",
        "print(\"1. Verify the wrapper cell has been re-run after fixes\")\n",
        "print(\"2. Check if loss is actually changing during training\")\n",
        "print(\"3. Try increasing learning rate to 2e-4 or 5e-4\")\n",
        "print(\"4. Verify Double DQN is working (check train_step method)\")\n",
        "print(\"5. Consider resetting and starting fresh with improved agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3ed9cfaf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Enhanced DQN Agent ready with better diagnostics!\n"
          ]
        }
      ],
      "source": [
        "# üõ†Ô∏è FIXED VERSION: Enhanced training with better diagnostics\n",
        "# Run this if your agent isn't learning after 1M+ steps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "# Enhanced ImprovedDQNAgent with better diagnostics\n",
        "class EnhancedDQNAgent:\n",
        "    \"\"\"\n",
        "    Enhanced DQN Agent with:\n",
        "    - Better learning rate schedule\n",
        "    - Gradient norm monitoring\n",
        "    - Q-value diagnostics\n",
        "    - More robust training\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_shape,\n",
        "        n_actions,\n",
        "        device='cuda',\n",
        "        lr=1e-4,  # Slightly higher learning rate\n",
        "        gamma=0.99,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=300000,  \n",
        "        target_update_freq=10000,\n",
        "        buffer_size=100000,\n",
        "        batch_size=32,\n",
        "        optimistic_init=1.0  # More conservative initialization\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.batch_size = batch_size\n",
        "        self.steps = 0\n",
        "        self.optimistic_init = optimistic_init\n",
        "        \n",
        "        # Create Q-network and target network\n",
        "        n_frames = state_shape[0]\n",
        "        self.q_network = Model(n_actions, n_frames).to(device)\n",
        "        self.target_network = Model(n_actions, n_frames).to(device)\n",
        "        \n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        self.target_network.eval()\n",
        "        \n",
        "        # Optimizer with better learning rate\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, eps=1e-8)\n",
        "        \n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        \n",
        "        # Diagnostics\n",
        "        self.loss_history = []\n",
        "        self.q_value_history = []\n",
        "    \n",
        "    def get_epsilon(self):\n",
        "        \"\"\"Calculate current epsilon with linear decay.\"\"\"\n",
        "        if self.steps < self.epsilon_decay:\n",
        "            return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
        "                   (1 - self.steps / self.epsilon_decay)\n",
        "        else:\n",
        "            return self.epsilon_end\n",
        "    \n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if training and random.random() < self.get_epsilon():\n",
        "            return random.randrange(self.n_actions)\n",
        "        \n",
        "        # Ensure state is correct shape\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0\n",
        "        else:\n",
        "            state_tensor = state.unsqueeze(0) if len(state.shape) == 3 else state\n",
        "            state_tensor = state_tensor.to(self.device) / 255.0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            q_values_t = self.q_network(state_tensor)        # shape (1, n_actions)\n",
        "            q_values = q_values_t[0].detach().cpu().numpy()  # shape (n_actions,)\n",
        "\n",
        "            max_q = q_values.max()\n",
        "            best = np.flatnonzero(q_values == max_q)         # all max actions\n",
        "            action = int(np.random.choice(best))             # break ties randomly\n",
        "\n",
        "            \n",
        "            # Store Q-values for diagnostics\n",
        "            if self.steps % 1000 == 0:\n",
        "                self.q_value_history.append(q_values.mean().item())\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        # reward clipping is fine\n",
        "        reward = np.clip(reward, -1.0, 1.0)\n",
        "\n",
        "        # IMPORTANT: store raw uint8 arrays (no torch, no /255 here)\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "        self.steps += 1\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"Perform one training step with Double DQN.\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        # Sample batch\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(\n",
        "        self.batch_size, self.device\n",
        "        )\n",
        "        # states/next_states are already float32 on device in [0,1]\n",
        "        \n",
        "        # Compute Q(s, a)\n",
        "        q_values = self.q_network(states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Double DQN\n",
        "        with torch.no_grad():\n",
        "            next_q_values_main = self.q_network(next_states)\n",
        "            next_actions = next_q_values_main.argmax(1)\n",
        "            next_q_values_target = self.target_network(next_states)\n",
        "            next_q_value = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.smooth_l1_loss(q_value, target_q_value)\n",
        "\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        if self.steps % self.target_update_freq == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        \n",
        "        # Store diagnostics\n",
        "        loss_item = loss.item()\n",
        "        self.loss_history.append(loss_item)\n",
        "        \n",
        "        # Print diagnostics every 10k steps\n",
        "        if self.steps % 10000 == 0 and len(self.loss_history) > 0:\n",
        "            avg_loss = np.mean(self.loss_history[-100:])\n",
        "            print(f\"   [Step {self.steps}] Loss: {avg_loss:.4f}, Epsilon: {self.get_epsilon():.3f}, \"\n",
        "                  f\"Q-mean: {np.mean(self.q_value_history[-10:]) if len(self.q_value_history) > 0 else 'N/A':.2f}\")\n",
        "        \n",
        "        return loss_item\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the model.\"\"\"\n",
        "        torch.save({\n",
        "            'q_network': self.q_network.state_dict(),\n",
        "            'target_network': self.target_network.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'steps': self.steps,\n",
        "            'loss_history': self.loss_history[-1000:],  # Save recent loss history\n",
        "        }, filepath)\n",
        "    \n",
        "    def load(self, filepath):\n",
        "        \"\"\"Load the model.\"\"\"\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
        "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.steps = checkpoint['steps']\n",
        "        self.loss_history = checkpoint.get('loss_history', [])\n",
        "\n",
        "print(\"‚úÖ Enhanced DQN Agent ready with better diagnostics!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "61acea6e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action meanings: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "State shape: (4, 84, 84)\n",
            "Number of actions: 3\n",
            "Using device: cuda\n",
            "\n",
            "‚úÖ Initial state shape: (4, 84, 84), dtype: uint8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 50000/6000000 [00:20<41:16, 2403.04it/s, epsilon=0.901, loss=0.0002, lr=1.00e-04, avg_reward=-20.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 50,000:\n",
            "   Avg reward (last 10): -20.20\n",
            "   Avg episode length: 1049.0\n",
            "   Epsilon: 0.901\n",
            "\n",
            "üß™ Greedy eval @ step 50,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 50124/6000000 [00:30<22:45:15, 72.63it/s, epsilon=0.901, loss=0.0297, lr=1.00e-04, avg_reward=-20.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3793})\n",
            "Avg Q mean/std: 0.009 / 0.007\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|‚ñè         | 100000/6000000 [01:57<3:17:59, 496.64it/s, epsilon=0.802, loss=0.0161, lr=1.00e-04, avg_reward=-20.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 100,000:\n",
            "   Avg reward (last 10): -20.80\n",
            "   Avg episode length: 911.5\n",
            "   Epsilon: 0.802\n",
            "\n",
            "üß™ Greedy eval @ step 100,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|‚ñè         | 100096/6000000 [02:10<85:07:27, 19.25it/s, epsilon=0.802, loss=0.0001, lr=1.00e-04, avg_reward=-20.8] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({1: 3799})\n",
            "Avg Q mean/std: -0.031 / 0.003\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|‚ñé         | 150000/6000000 [04:01<4:12:10, 386.64it/s, epsilon=0.703, loss=0.0003, lr=1.00e-04, avg_reward=-20.6] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 150,000:\n",
            "   Avg reward (last 10): -20.60\n",
            "   Avg episode length: 894.4\n",
            "   Epsilon: 0.703\n",
            "\n",
            "üß™ Greedy eval @ step 150,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   3%|‚ñé         | 150072/6000000 [04:21<191:56:24,  8.47it/s, epsilon=0.703, loss=0.0003, lr=1.00e-04, avg_reward=-20.5]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3797})\n",
            "Avg Q mean/std: -0.011 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   3%|‚ñé         | 200000/6000000 [06:52<5:28:30, 294.26it/s, epsilon=0.604, loss=0.0151, lr=1.00e-04, avg_reward=-20.8] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Model saved at step 200,000\n",
            "   Recent avg reward: -20.80\n",
            "\n",
            "üìä Step 200,000:\n",
            "   Avg reward (last 10): -20.80\n",
            "   Avg episode length: 897.9\n",
            "   Epsilon: 0.604\n",
            "\n",
            "üß™ Greedy eval @ step 200,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   3%|‚ñé         | 200056/6000000 [07:17<290:04:46,  5.55it/s, epsilon=0.604, loss=0.0302, lr=1.00e-04, avg_reward=-20.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3802})\n",
            "Avg Q mean/std: -0.008 / 0.003\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|‚ñç         | 250000/6000000 [10:27<6:37:10, 241.29it/s, epsilon=0.505, loss=0.0302, lr=1.00e-04, avg_reward=-20.8] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 250,000:\n",
            "   Avg reward (last 10): -20.80\n",
            "   Avg episode length: 851.6\n",
            "   Epsilon: 0.505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|‚ñç         | 250000/6000000 [10:40<6:37:10, 241.29it/s, epsilon=0.505, loss=0.0302, lr=1.00e-04, avg_reward=-20.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 250,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|‚ñç         | 250048/6000000 [10:55<435:06:43,  3.67it/s, epsilon=0.505, loss=0.0300, lr=1.00e-04, avg_reward=-20.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3801})\n",
            "Avg Q mean/std: -0.022 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|‚ñå         | 300000/6000000 [14:39<7:39:20, 206.81it/s, epsilon=0.406, loss=0.0151, lr=1.00e-04, avg_reward=-20.7] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 300,000:\n",
            "   Avg reward (last 10): -20.70\n",
            "   Avg episode length: 975.7\n",
            "   Epsilon: 0.406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|‚ñå         | 300000/6000000 [14:50<7:39:20, 206.81it/s, epsilon=0.406, loss=0.0151, lr=1.00e-04, avg_reward=-20.7]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 300,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|‚ñå         | 300040/6000000 [15:15<622:34:14,  2.54it/s, epsilon=0.406, loss=0.0003, lr=1.00e-04, avg_reward=-20.7]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3806})\n",
            "Avg Q mean/std: -0.023 / 0.003\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|‚ñå         | 350000/6000000 [19:32<9:12:23, 170.47it/s, epsilon=0.307, loss=0.0004, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 350,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 831.9\n",
            "   Epsilon: 0.307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|‚ñå         | 350000/6000000 [19:51<9:12:23, 170.47it/s, epsilon=0.307, loss=0.0004, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 350,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|‚ñå         | 350036/6000000 [20:12<789:30:09,  1.99it/s, epsilon=0.307, loss=0.0151, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3799})\n",
            "Avg Q mean/std: -0.013 / 0.003\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|‚ñã         | 400000/6000000 [25:04<9:46:32, 159.12it/s, epsilon=0.208, loss=0.0005, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Model saved at step 400,000\n",
            "   Recent avg reward: -21.00\n",
            "\n",
            "üìä Step 400,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 915.2\n",
            "   Epsilon: 0.208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|‚ñã         | 400000/6000000 [25:21<9:46:32, 159.12it/s, epsilon=0.208, loss=0.0005, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 400,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|‚ñã         | 400032/6000000 [25:48<1024:02:54,  1.52it/s, epsilon=0.208, loss=0.0151, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3798})\n",
            "Avg Q mean/std: -0.017 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 450000/6000000 [31:15<10:43:07, 143.83it/s, epsilon=0.109, loss=0.0153, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 450,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 838.7\n",
            "   Epsilon: 0.109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 450000/6000000 [31:31<10:43:07, 143.83it/s, epsilon=0.109, loss=0.0153, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 450,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 450028/6000000 [32:03<1190:46:29,  1.29it/s, epsilon=0.109, loss=0.0153, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3798})\n",
            "Avg Q mean/std: -0.016 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 500000/6000000 [38:04<11:20:01, 134.80it/s, epsilon=0.010, loss=0.0004, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 500,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 790.3\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 500000/6000000 [38:21<11:20:01, 134.80it/s, epsilon=0.010, loss=0.0004, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 500,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|‚ñä         | 500024/6000000 [38:57<1226:18:41,  1.25it/s, epsilon=0.010, loss=0.0004, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3799})\n",
            "Avg Q mean/std: -0.014 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   9%|‚ñâ         | 550000/6000000 [45:31<12:22:04, 122.40it/s, epsilon=0.010, loss=0.0003, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 550,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 759.9\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   9%|‚ñâ         | 550000/6000000 [45:42<12:22:04, 122.40it/s, epsilon=0.010, loss=0.0003, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 550,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   9%|‚ñâ         | 550020/6000000 [46:28<1451:13:28,  1.04it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3788})\n",
            "Avg Q mean/std: -0.022 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|‚ñà         | 600000/6000000 [53:38<13:09:37, 113.98it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Model saved at step 600,000\n",
            "   Recent avg reward: -21.00\n",
            "\n",
            "üìä Step 600,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 813.8\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|‚ñà         | 600000/6000000 [53:52<13:09:37, 113.98it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 600,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|‚ñà         | 600020/6000000 [54:36<1799:24:43,  1.20s/it, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({1: 3801})\n",
            "Avg Q mean/std: -0.039 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|‚ñà         | 650000/6000000 [1:02:18<13:51:56, 107.18it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 650,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 783.5\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|‚ñà         | 650000/6000000 [1:02:33<13:51:56, 107.18it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 650,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|‚ñà         | 650020/6000000 [1:03:24<2239:02:45,  1.51s/it, epsilon=0.010, loss=0.0299, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3804})\n",
            "Avg Q mean/std: -0.013 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|‚ñà‚ñè        | 700000/6000000 [1:11:42<15:06:02, 97.49it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 700,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 819.6\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|‚ñà‚ñè        | 700000/6000000 [1:11:53<15:06:02, 97.49it/s, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 700,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|‚ñà‚ñè        | 700016/6000000 [1:12:52<2335:47:17,  1.59s/it, epsilon=0.010, loss=0.0005, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3796})\n",
            "Avg Q mean/std: -0.017 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|‚ñà‚ñé        | 750000/6000000 [1:21:43<17:19:45, 84.15it/s, epsilon=0.010, loss=0.0001, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 750,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 789.8\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|‚ñà‚ñé        | 750000/6000000 [1:21:54<17:19:45, 84.15it/s, epsilon=0.010, loss=0.0001, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 750,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|‚ñà‚ñé        | 750016/6000000 [1:23:05<3059:11:59,  2.10s/it, epsilon=0.010, loss=0.0001, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3797})\n",
            "Avg Q mean/std: -0.013 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|‚ñà‚ñé        | 800000/6000000 [1:32:33<17:10:51, 84.07it/s, epsilon=0.010, loss=0.0000, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Model saved at step 800,000\n",
            "   Recent avg reward: -21.00\n",
            "\n",
            "üìä Step 800,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 784.3\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|‚ñà‚ñé        | 800000/6000000 [1:32:44<17:10:51, 84.07it/s, epsilon=0.010, loss=0.0000, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 800,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|‚ñà‚ñé        | 800016/6000000 [1:33:49<2975:00:53,  2.06s/it, epsilon=0.010, loss=0.0155, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({1: 3798})\n",
            "Avg Q mean/std: -0.024 / 0.000\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|‚ñà‚ñç        | 850000/6000000 [1:44:00<18:02:16, 79.31it/s, epsilon=0.010, loss=0.0449, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 850,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 789.9\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|‚ñà‚ñç        | 850000/6000000 [1:44:15<18:02:16, 79.31it/s, epsilon=0.010, loss=0.0449, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 850,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|‚ñà‚ñç        | 850016/6000000 [1:45:26<3296:55:11,  2.30s/it, epsilon=0.010, loss=0.0156, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({2: 3796})\n",
            "Avg Q mean/std: -0.009 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|‚ñà‚ñå        | 900000/6000000 [1:56:19<18:05:14, 78.32it/s, epsilon=0.010, loss=0.0297, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 900,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 759.6\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|‚ñà‚ñå        | 900000/6000000 [1:56:36<18:05:14, 78.32it/s, epsilon=0.010, loss=0.0297, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 900,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|‚ñà‚ñå        | 900012/6000000 [1:57:57<4225:04:52,  2.98s/it, epsilon=0.010, loss=0.0298, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3796})\n",
            "Avg Q mean/std: -0.033 / 0.002\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|‚ñà‚ñå        | 950000/6000000 [2:09:31<20:22:25, 68.85it/s, epsilon=0.010, loss=0.0003, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 950,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 818.3\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|‚ñà‚ñå        | 950000/6000000 [2:09:46<20:22:25, 68.85it/s, epsilon=0.010, loss=0.0003, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 950,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|‚ñà‚ñå        | 950012/6000000 [2:11:08<3569:34:05,  2.54s/it, epsilon=0.010, loss=0.0152, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({0: 3800})\n",
            "Avg Q mean/std: -0.024 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  17%|‚ñà‚ñã        | 1000000/6000000 [2:24:07<19:47:12, 70.19it/s, epsilon=0.010, loss=0.0154, lr=1.00e-04, avg_reward=-21.0] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Model saved at step 1,000,000\n",
            "   Recent avg reward: -21.00\n",
            "\n",
            "üìä Step 1,000,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 796.0\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  17%|‚ñà‚ñã        | 1000000/6000000 [2:24:17<19:47:12, 70.19it/s, epsilon=0.010, loss=0.0154, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Greedy eval @ step 1,000,000: mean=-21.00 ¬± 0.00  (scores=[-21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0, -21.0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  17%|‚ñà‚ñã        | 1000012/6000000 [2:25:42<3480:11:46,  2.51s/it, epsilon=0.010, loss=0.0000, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action counts: Counter({1: 3790})\n",
            "Avg Q mean/std: -0.026 / 0.001\n",
            "   ‚è≥ Still exploring...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  18%|‚ñà‚ñä        | 1050000/6000000 [2:38:52<27:24:51, 50.16it/s, epsilon=0.010, loss=0.0301, lr=1.00e-04, avg_reward=-21.0]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Step 1,050,000:\n",
            "   Avg reward (last 10): -21.00\n",
            "   Avg episode length: 813.1\n",
            "   Epsilon: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  18%|‚ñà‚ñä        | 1050000/6000000 [2:39:08<27:24:51, 50.16it/s, epsilon=0.010, loss=0.0301, lr=1.00e-04, avg_reward=-21.0]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 220\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Avg episode length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_length\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    218\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.get_epsilon()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m mean_r, std_r, all_r = \u001b[43mevaluate_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müß™ Greedy eval @ step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_r\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ¬± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_r\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (scores=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_r\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m scores, counts, qmean, qstd = greedy_eval_diagnostics(agent, eval_env, n_episodes=\u001b[32m5\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mevaluate_greedy\u001b[39m\u001b[34m(agent, env, n_episodes)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     18\u001b[39m     a = agent.select_action(s, training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     s, r, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m     21\u001b[39m     ep_r += r\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\core.py:636\u001b[39m, in \u001b[36mActionWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    633\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    634\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    635\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the :attr:`env` :meth:`env.step` using the modified ``action`` from :meth:`self.action`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:425\u001b[39m, in \u001b[36mFrameStackObservation.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    415\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    416\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    417\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[32m    418\u001b[39m \n\u001b[32m    419\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.obs_queue.append(obs)\n\u001b[32m    428\u001b[39m     updated_obs = deepcopy(\n\u001b[32m    429\u001b[39m         concatenate(\u001b[38;5;28mself\u001b[39m.env.observation_space, \u001b[38;5;28mself\u001b[39m.obs_queue, \u001b[38;5;28mself\u001b[39m.stacked_obs)\n\u001b[32m    430\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:185\u001b[39m, in \u001b[36mAtariPreprocessing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    183\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m             \u001b[38;5;28mself\u001b[39m.ale.getScreenRGB(\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, total_reward, terminated, truncated, info\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sophi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gymnasium\\wrappers\\atari_preprocessing.py:220\u001b[39m, in \u001b[36mAtariPreprocessing._get_obs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    216\u001b[39m     np.maximum(\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m1\u001b[39m], out=\u001b[38;5;28mself\u001b[39m.obs_buffer[\u001b[32m0\u001b[39m])\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m obs = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobs_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscreen_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mINTER_AREA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scale_obs:\n\u001b[32m    227\u001b[39m     obs = np.asarray(obs, dtype=np.float32) / \u001b[32m255.0\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# üöÄ TRAINING with Enhanced DQN Agent\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "def evaluate_greedy(agent, env, n_episodes=10):\n",
        "    rewards = []\n",
        "    for _ in range(n_episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        ep_r = 0.0\n",
        "        while not done:\n",
        "            a = agent.select_action(s, training=False)\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_r += r\n",
        "        rewards.append(ep_r)\n",
        "    return float(np.mean(rewards)), float(np.std(rewards)), rewards\n",
        "\n",
        "def greedy_eval_diagnostics(agent, env, n_episodes=5):\n",
        "    action_counts = Counter()\n",
        "    scores = []\n",
        "    q_means = []\n",
        "    q_stds = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        ep_r = 0.0\n",
        "\n",
        "        while not done:\n",
        "            a = agent.select_action(s, training=False)\n",
        "            action_counts[a] += 1\n",
        "\n",
        "            with torch.no_grad():\n",
        "                st = torch.as_tensor(s, dtype=torch.float32, device=agent.device).unsqueeze(0) / 255.0\n",
        "                q = agent.q_network(st)[0]\n",
        "                q_means.append(float(q.mean().item()))\n",
        "                q_stds.append(float(q.std().item()))\n",
        "\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_r += r\n",
        "\n",
        "        scores.append(ep_r)\n",
        "\n",
        "    return scores, action_counts, float(np.mean(q_means)), float(np.mean(q_stds))\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\n",
        "        \"ALE/Pong-v5\",\n",
        "        frameskip=1,\n",
        "        repeat_action_probability=0.0,\n",
        "        full_action_space=False\n",
        "    )\n",
        "\n",
        "    env = AtariPreprocessing(\n",
        "        env,\n",
        "        noop_max=30,\n",
        "        frame_skip=4,\n",
        "        terminal_on_life_loss=False,\n",
        "        screen_size=84,\n",
        "        grayscale_obs=True,\n",
        "        grayscale_newaxis=False,\n",
        "        scale_obs=False\n",
        "    )\n",
        "\n",
        "    env = FireReset(env)\n",
        "    env = FrameStackObservation(env, stack_size=4)\n",
        "\n",
        "    # Reduced actions: NOOP, RIGHT, LEFT\n",
        "    env = ReducedActionSet(env, allowed_actions=[0, 2, 3])\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "# Create environment with preprocessing\n",
        "env = gym.make(\n",
        "    \"ALE/Pong-v5\",\n",
        "    frameskip=1,\n",
        "    repeat_action_probability=0.0,\n",
        "    full_action_space=False\n",
        ")\n",
        "\n",
        "env = AtariPreprocessing(\n",
        "    env,\n",
        "    noop_max=30,\n",
        "    frame_skip=4,\n",
        "    terminal_on_life_loss=False,\n",
        "    screen_size=84,\n",
        "    grayscale_obs=True,\n",
        "    grayscale_newaxis=False,\n",
        "    scale_obs=False\n",
        ")\n",
        "\n",
        "env = make_env()\n",
        "eval_env = make_env()\n",
        "\n",
        "print(\"Action meanings:\", env.unwrapped.get_action_meanings())\n",
        "\n",
        "# Get environment info\n",
        "state_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(f\"State shape: {state_shape}\")\n",
        "print(f\"Number of actions: {n_actions}\")\n",
        "\n",
        "# Create ENHANCED agent with better hyperparameters\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "agent = EnhancedDQNAgent(\n",
        "    state_shape=state_shape,\n",
        "    n_actions=n_actions,\n",
        "    device=device,\n",
        "    lr=1e-4,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=500_000,\n",
        "    target_update_freq=10_000,\n",
        "    buffer_size=1_000_000,\n",
        "    batch_size=32,\n",
        "    optimistic_init=1.0\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "total_steps = 6_000_000  # Train for 6M steps\n",
        "learning_starts = 50_000  # Start training after 50k steps\n",
        "train_freq = 4  # Train every 4 steps\n",
        "save_freq = 200_000  # Save every 200k steps\n",
        "eval_freq = 50_000  # Print stats every 50k steps\n",
        "\n",
        "# Statistics\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "current_episode_reward = 0\n",
        "current_episode_length = 0\n",
        "\n",
        "# Start training from scratch (always)\n",
        "model_path = \"models/dqn_pong_enhanced.pth\"\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Optionally delete previous model if you want to start completely fresh\n",
        "# Uncomment the next line if you want to delete any existing model:\n",
        "# if os.path.exists(model_path):\n",
        "#     os.remove(model_path)\n",
        "#     print(f\"üóëÔ∏è  Deleted previous model at {model_path}\")\n",
        "\n",
        "# Verify state shape is correct\n",
        "state, info = env.reset()\n",
        "print(f\"\\n‚úÖ Initial state shape: {state.shape}, dtype: {state.dtype}\")\n",
        "if state.shape != state_shape:\n",
        "    raise ValueError(f\"State shape mismatch! Expected {state_shape}, got {state.shape}\")\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(total_steps), desc=\"Training\")\n",
        "\n",
        "for step in range(total_steps):\n",
        "    # Select action\n",
        "    action = agent.select_action(state, training=True)\n",
        "    \n",
        "    # Take step\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    # Store transition\n",
        "    agent.store_transition(state, action, reward, next_state, done)\n",
        "    \n",
        "    # Update statistics\n",
        "    current_episode_reward += reward\n",
        "    current_episode_length += 1\n",
        "    \n",
        "    # Train\n",
        "    if step >= learning_starts and step % train_freq == 0:\n",
        "        loss = agent.train_step()\n",
        "        if loss is not None:\n",
        "            current_lr = agent.optimizer.param_groups[0]['lr']\n",
        "            pbar.set_postfix({\n",
        "                'epsilon': f'{agent.get_epsilon():.3f}',\n",
        "                'loss': f'{loss:.4f}',\n",
        "                'lr': f'{current_lr:.2e}',\n",
        "                'avg_reward': f'{np.mean(episode_rewards[-10:]):.1f}' if len(episode_rewards) >= 10 else 'N/A'\n",
        "            })\n",
        "    \n",
        "    # Handle episode end\n",
        "    if done:\n",
        "        episode_rewards.append(current_episode_reward)\n",
        "        episode_lengths.append(current_episode_length)\n",
        "        current_episode_reward = 0\n",
        "        current_episode_length = 0\n",
        "        state, info = env.reset()\n",
        "    else:\n",
        "        state = next_state\n",
        "    \n",
        "    # Save model periodically\n",
        "    if step > 0 and step % save_freq == 0:\n",
        "        agent.save(model_path)\n",
        "        print(f\"\\nüíæ Model saved at step {step:,}\")\n",
        "        if len(episode_rewards) >= 10:\n",
        "            recent_avg = np.mean(episode_rewards[-10:])\n",
        "            print(f\"   Recent avg reward: {recent_avg:.2f}\")\n",
        "    \n",
        "    # Evaluate and show progress\n",
        "    if step > 0 and step % eval_freq == 0 and len(episode_rewards) >= 10:\n",
        "        avg_reward = np.mean(episode_rewards[-10:])\n",
        "        avg_length = np.mean(episode_lengths[-10:])\n",
        "        print(f\"\\nüìä Step {step:,}:\")\n",
        "        print(f\"   Avg reward (last 10): {avg_reward:.2f}\")\n",
        "        print(f\"   Avg episode length: {avg_length:.1f}\")\n",
        "        print(f\"   Epsilon: {agent.get_epsilon():.3f}\")\n",
        "\n",
        "        mean_r, std_r, all_r = evaluate_greedy(agent, eval_env, n_episodes=10)\n",
        "        print(f\"\\nüß™ Greedy eval @ step {step:,}: mean={mean_r:.2f} ¬± {std_r:.2f}  (scores={all_r})\")\n",
        "\n",
        "        scores, counts, qmean, qstd = greedy_eval_diagnostics(agent, eval_env, n_episodes=5)\n",
        "        print(\"Greedy action counts:\", counts)\n",
        "        print(f\"Avg Q mean/std: {qmean:.3f} / {qstd:.3f}\")\n",
        "        \n",
        "        # Show learning progress\n",
        "        if avg_reward > 10:\n",
        "            print(\"   üéâüéâüéâ EXCELLENT! Agent is winning consistently!\")\n",
        "        elif avg_reward > 0:\n",
        "            print(\"   üéâüéâ BREAKTHROUGH! Agent is winning!\")\n",
        "        elif avg_reward > -10:\n",
        "            print(\"   üéØ Great progress! Agent is learning!\")\n",
        "        elif avg_reward > -15:\n",
        "            print(\"   üìà Starting to improve!\")\n",
        "        elif avg_reward > -19:\n",
        "            print(\"   üìä Better than random, keep going!\")\n",
        "        else:\n",
        "            print(\"   ‚è≥ Still exploring...\")\n",
        "    \n",
        "    pbar.update(1)\n",
        "\n",
        "# Final save\n",
        "agent.save(model_path)\n",
        "env.close()\n",
        "pbar.close()\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete! Model saved to {model_path}\")\n",
        "print(f\"Total episodes: {len(episode_rewards)}\")\n",
        "if len(episode_rewards) > 0:\n",
        "    print(f\"Final average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "    print(f\"Best average reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
        "    print(f\"Best single episode: {max(episode_rewards):.2f}\")\n",
        "    \n",
        "    # Final assessment\n",
        "    final_avg = np.mean(episode_rewards[-10:])\n",
        "    if final_avg > 10:\n",
        "        print(\"üåüüåüüåü EXCELLENT! Agent mastered the game!\")\n",
        "    elif final_avg > 0:\n",
        "        print(\"üéØ SUCCESS! Agent is winning more than losing!\")\n",
        "    elif final_avg > -10:\n",
        "        print(\"üìà Good progress! Agent is learning!\")\n",
        "    elif final_avg > -15:\n",
        "        print(\"üìä Some improvement, but needs more training\")\n",
        "    else:\n",
        "        print(\"‚è≥ Still needs work - may need hyperparameter tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "31562234",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All classes and imports are ready!\n",
            "‚úÖ You can now run the training cell to start training!\n"
          ]
        }
      ],
      "source": [
        "# Quick verification - Run this before training to make sure everything is set up\n",
        "try:\n",
        "    # Check if all classes are defined\n",
        "    assert 'Model' in globals(), \"Model class not found - run the Model cell first!\"\n",
        "    assert 'ReplayBuffer' in globals(), \"ReplayBuffer class not found - run the ReplayBuffer cell first!\"\n",
        "    assert 'PreprocessAtari' in globals(), \"PreprocessAtari class not found - run the preprocessing cell first!\"\n",
        "    assert 'FrameStack' in globals(), \"FrameStack class not found - run the preprocessing cell first!\"\n",
        "    assert 'EnhancedDQNAgent' in globals(), \"DQNAgent class not found - run the DQNAgent cell first!\"\n",
        "    \n",
        "    # Check imports\n",
        "    import torch\n",
        "    import gymnasium as gym\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    print(\"‚úÖ All classes and imports are ready!\")\n",
        "    print(\"‚úÖ You can now run the training cell to start training!\")\n",
        "except AssertionError as e:\n",
        "    print(f\"‚ùå {e}\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing import: {e}\")\n",
        "    print(\"Make sure you've run all the setup cells in order.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f2b5c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ RESUME TRAINING: Continue from saved checkpoint\n",
        "# This cell loads an existing model and continues training from where it left off\n",
        "\n",
        "import ale_py\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import gc  # For garbage collection to prevent memory issues\n",
        "import torch\n",
        "\n",
        "# Create environment with preprocessing\n",
        "env = gym.make(\"ALE/Pong-v5\")\n",
        "env = PreprocessAtari(env)\n",
        "env = FrameStack(env, n_frames=4)\n",
        "\n",
        "# Get environment info\n",
        "state_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(f\"State shape: {state_shape}\")\n",
        "print(f\"Number of actions: {n_actions}\")\n",
        "\n",
        "# Create agent\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "agent = EnhancedDQNAgent(\n",
        "    state_shape=state_shape,\n",
        "    n_actions=n_actions,\n",
        "    device=device,\n",
        "    lr=2e-4,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.01,\n",
        "    epsilon_decay=1000000,\n",
        "    target_update_freq=1000,\n",
        "    buffer_size=100000,\n",
        "    batch_size=32,\n",
        "    optimistic_init=1.0\n",
        ")\n",
        "\n",
        "# Load the saved model checkpoint\n",
        "model_path = \"models/dqn_pong_enhanced_800k.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    agent.load(model_path)\n",
        "    print(f\"\\n‚úÖ Loaded model from {model_path}\")\n",
        "    print(f\"   Resuming from step {agent.steps:,}\")\n",
        "    if len(agent.loss_history) > 0:\n",
        "        recent_loss = np.mean(agent.loss_history[-100:]) if len(agent.loss_history) >= 100 else np.mean(agent.loss_history)\n",
        "        print(f\"   Recent average loss: {recent_loss:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Model file not found at {model_path}\")\n",
        "    print(\"   Please check the file path or use the fresh training cell instead.\")\n",
        "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "# Training parameters\n",
        "total_steps = 6_000_000  # Continue training up to 6M steps total\n",
        "learning_starts = 10_000  # Already passed this\n",
        "train_freq = 4\n",
        "save_freq = 200_000  # Save every 200k steps\n",
        "eval_freq = 50_000  # Print stats every 50k steps\n",
        "\n",
        "# Statistics - will continue tracking from here\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "current_episode_reward = 0\n",
        "current_episode_length = 0\n",
        "\n",
        "# Verify state shape\n",
        "state, info = env.reset()\n",
        "print(f\"\\n‚úÖ Initial state shape: {state.shape}, dtype: {state.dtype}\")\n",
        "if state.shape != state_shape:\n",
        "    raise ValueError(f\"State shape mismatch! Expected {state_shape}, got {state.shape}\")\n",
        "\n",
        "# Training loop - resume from agent.steps\n",
        "print(f\"\\nüöÄ Resuming training from step {agent.steps:,} to {total_steps:,}\")\n",
        "print(f\"   Total steps remaining: {total_steps - agent.steps:,}\")\n",
        "\n",
        "pbar = tqdm(range(agent.steps, total_steps), desc=\"Training (Resumed)\", initial=agent.steps)\n",
        "\n",
        "try:\n",
        "    for step in range(agent.steps, total_steps):\n",
        "        # Select action\n",
        "        action = agent.select_action(state, training=True)\n",
        "        \n",
        "        # Take step\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        # Store transition\n",
        "        agent.store_transition(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Update statistics\n",
        "        current_episode_reward += reward\n",
        "        current_episode_length += 1\n",
        "        \n",
        "        # Train\n",
        "        if step >= learning_starts and step % train_freq == 0:\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                current_lr = agent.optimizer.param_groups[0]['lr']\n",
        "                pbar.set_postfix({\n",
        "                    'epsilon': f'{agent.get_epsilon():.3f}',\n",
        "                    'loss': f'{loss:.4f}',\n",
        "                    'lr': f'{current_lr:.2e}',\n",
        "                    'avg_reward': f'{np.mean(episode_rewards[-10:]):.1f}' if len(episode_rewards) >= 10 else 'N/A'\n",
        "                })\n",
        "        \n",
        "        # Handle episode end\n",
        "        if done:\n",
        "            episode_rewards.append(current_episode_reward)\n",
        "            episode_lengths.append(current_episode_length)\n",
        "            current_episode_reward = 0\n",
        "            current_episode_length = 0\n",
        "            state, info = env.reset()\n",
        "        else:\n",
        "            state = next_state\n",
        "        \n",
        "        # Save model periodically\n",
        "        if step > 0 and step % save_freq == 0:\n",
        "            # Save with step number in filename to keep multiple checkpoints\n",
        "            checkpoint_path = f\"models/dqn_pong_enhanced_{step//1000}k.pth\"\n",
        "            agent.save(checkpoint_path)\n",
        "            print(f\"\\nüíæ Model saved at step {step:,} to {checkpoint_path}\")\n",
        "            if len(episode_rewards) >= 10:\n",
        "                recent_avg = np.mean(episode_rewards[-10:])\n",
        "                print(f\"   Recent avg reward: {recent_avg:.2f}\")\n",
        "            \n",
        "            # Force garbage collection after saving to free memory\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # Evaluate and show progress\n",
        "        if step > 0 and step % eval_freq == 0 and len(episode_rewards) >= 10:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            avg_length = np.mean(episode_lengths[-10:])\n",
        "            print(f\"\\nüìä Step {step:,}:\")\n",
        "            print(f\"   Avg reward (last 10): {avg_reward:.2f}\")\n",
        "            print(f\"   Avg episode length: {avg_length:.1f}\")\n",
        "            print(f\"   Epsilon: {agent.get_epsilon():.3f}\")\n",
        "            \n",
        "            # Show learning progress\n",
        "            if avg_reward > 10:\n",
        "                print(\"   üéâüéâüéâ EXCELLENT! Agent is winning consistently!\")\n",
        "            elif avg_reward > 0:\n",
        "                print(\"   üéâüéâ BREAKTHROUGH! Agent is winning!\")\n",
        "            elif avg_reward > -10:\n",
        "                print(\"   üéØ Great progress! Agent is learning!\")\n",
        "            elif avg_reward > -15:\n",
        "                print(\"   üìà Starting to improve!\")\n",
        "            elif avg_reward > -19:\n",
        "                print(\"   üìä Better than random, keep going!\")\n",
        "            else:\n",
        "                print(\"   ‚è≥ Still exploring...\")\n",
        "        \n",
        "        pbar.update(1)\n",
        "        \n",
        "        # Periodic memory cleanup to prevent MemoryError\n",
        "        if step % 10000 == 0:\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "except MemoryError as e:\n",
        "    print(f\"\\n‚ö†Ô∏è  MemoryError occurred at step {step:,}\")\n",
        "    print(f\"   Saving model before exit...\")\n",
        "    emergency_path = f\"models/dqn_pong_enhanced_emergency_{step//1000}k.pth\"\n",
        "    agent.save(emergency_path)\n",
        "    print(f\"   ‚úÖ Emergency save completed: {emergency_path}\")\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    # Final save\n",
        "    final_path = f\"models/dqn_pong_enhanced_final_{agent.steps//1000}k.pth\"\n",
        "    agent.save(final_path)\n",
        "    env.close()\n",
        "    pbar.close()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Training session ended. Model saved to {final_path}\")\n",
        "    print(f\"Final step: {agent.steps:,}\")\n",
        "    print(f\"Total episodes completed: {len(episode_rewards)}\")\n",
        "    if len(episode_rewards) > 0:\n",
        "        print(f\"Final average reward (last 100): {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "        print(f\"Best average reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
        "        print(f\"Best single episode: {max(episode_rewards):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b44192bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded model from models/dqn_pong.pth\n",
            "   Model was trained for 100000 steps\n",
            "\n",
            "Running 10 evaluation episodes (no exploration, greedy policy)...\n",
            "Episode 1: Reward = -21.0, Length = 764\n",
            "Episode 2: Reward = -21.0, Length = 764\n",
            "Episode 3: Reward = -21.0, Length = 764\n",
            "Episode 4: Reward = -21.0, Length = 764\n",
            "Episode 5: Reward = -21.0, Length = 764\n",
            "Episode 6: Reward = -21.0, Length = 764\n",
            "Episode 7: Reward = -21.0, Length = 764\n",
            "Episode 8: Reward = -21.0, Length = 764\n",
            "Episode 9: Reward = -21.0, Length = 764\n",
            "Episode 10: Reward = -21.0, Length = 764\n",
            "\n",
            "==================================================\n",
            "Evaluation Summary (10 episodes):\n",
            "  Average Reward: -21.00\n",
            "  Best Reward: -21.00\n",
            "  Worst Reward: -21.00\n",
            "  Average Length: 764.0 steps\n",
            "==================================================\n",
            "üìâ Status: Still playing randomly (needs more training)\n",
            "   ‚Üí Increase training to 1,000,000+ steps\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained agent\n",
        "import ale_py\n",
        "import numpy as np\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = gym.make(\"ALE/Pong-v5\")\n",
        "eval_env = PreprocessAtari(eval_env)\n",
        "eval_env = FrameStack(eval_env, n_frames=4)\n",
        "\n",
        "# Load the trained model\n",
        "model_path = \"models/dqn_pong.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    # Recreate agent with same parameters\n",
        "    state_shape = eval_env.observation_space.shape\n",
        "    n_actions = eval_env.action_space.n\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    eval_agent = DQNAgent(\n",
        "        state_shape=state_shape,\n",
        "        n_actions=n_actions,\n",
        "        device=device,\n",
        "        lr=1e-4,\n",
        "        gamma=0.99,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=10000,\n",
        "        target_update_freq=1000,\n",
        "        buffer_size=100000,\n",
        "        batch_size=32\n",
        "    )\n",
        "    eval_agent.load(model_path)\n",
        "    print(f\"‚úÖ Loaded model from {model_path}\")\n",
        "    print(f\"   Model was trained for {eval_agent.steps} steps\")\n",
        "else:\n",
        "    print(\"‚ùå No model found to evaluate\")\n",
        "    eval_env.close()\n",
        "\n",
        "# Run evaluation episodes\n",
        "n_eval_episodes = 10\n",
        "eval_rewards = []\n",
        "eval_lengths = []\n",
        "\n",
        "print(f\"\\nRunning {n_eval_episodes} evaluation episodes (no exploration, greedy policy)...\")\n",
        "for episode in range(n_eval_episodes):\n",
        "    state, info = eval_env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    \n",
        "    while True:\n",
        "        # Use greedy policy (no exploration)\n",
        "        action = eval_agent.select_action(state, training=False)\n",
        "        state, reward, terminated, truncated, info = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        episode_reward += reward\n",
        "        episode_length += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    eval_rewards.append(episode_reward)\n",
        "    eval_lengths.append(episode_length)\n",
        "    print(f\"Episode {episode+1}: Reward = {episode_reward:+.1f}, Length = {episode_length}\")\n",
        "\n",
        "eval_env.close()\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Evaluation Summary ({n_eval_episodes} episodes):\")\n",
        "print(f\"  Average Reward: {np.mean(eval_rewards):.2f}\")\n",
        "print(f\"  Best Reward: {np.max(eval_rewards):.2f}\")\n",
        "print(f\"  Worst Reward: {np.min(eval_rewards):.2f}\")\n",
        "print(f\"  Average Length: {np.mean(eval_lengths):.1f} steps\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Interpretation\n",
        "avg_reward = np.mean(eval_rewards)\n",
        "if avg_reward < -19:\n",
        "    print(\"üìâ Status: Still playing randomly (needs more training)\")\n",
        "    print(\"   ‚Üí Increase training to 1,000,000+ steps\")\n",
        "elif avg_reward < -10:\n",
        "    print(\"üìà Status: Starting to learn (showing some improvement)\")\n",
        "    print(\"   ‚Üí Continue training to see more improvement\")\n",
        "elif avg_reward < 0:\n",
        "    print(\"üéØ Status: Learning! (better than random)\")\n",
        "    print(\"   ‚Üí Keep training to reach positive rewards\")\n",
        "elif avg_reward < 10:\n",
        "    print(\"üèÜ Status: Playing well! (winning some games)\")\n",
        "    print(\"   ‚Üí Excellent progress!\")\n",
        "else:\n",
        "    print(\"üåü Status: Master level! (consistently winning)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5b6ac2fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    DQN Model for Atari games.\n",
        "    Takes stacked frames as input and outputs Q-values for each action.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_actions, n_frames=4):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_actions: Number of possible actions (e.g., 6 for Pong)\n",
        "            n_frames: Number of stacked frames (default: 4)\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        # Convolutional layers to process the image frames\n",
        "        self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        \n",
        "        # Calculate the size of the flattened feature map\n",
        "        # Input shape: (n_frames, 84, 84) after preprocessing (or 210x160x3 raw)\n",
        "        # After conv layers, we need to calculate the output size\n",
        "        # For standard Atari preprocessing (84x84), the output is 7x7x64\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.fc2 = nn.Linear(512, n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, n_frames, height, width)\n",
        "        \n",
        "        Returns:\n",
        "            Q-values for each action, shape (batch_size, n_actions)\n",
        "        \"\"\"\n",
        "        # Apply convolutional layers with ReLU activation\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        # Flatten the feature map\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Apply fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "codeCellConfig": {
        "lineNumbers": true
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
