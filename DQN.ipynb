{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN for Atari Pong\n",
    "\n",
    "Clean implementation organized into separate cells.\n",
    "\n",
    "**Run cells in order:**\n",
    "1. Imports\n",
    "2. Wrappers\n",
    "3. Model\n",
    "4. Replay Buffer\n",
    "5. DQN Agent\n",
    "6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gymnasium as gym\n",
    "import ale_py  # Registers ALE environments\n",
    "from collections import deque, namedtuple\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrappers working! Observation shape: (4, 84, 84), dtype: uint8\n",
      "   Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Wrappers for Atari preprocessing\n",
    "# Based on OpenAI Baselines but adapted for gymnasium\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"Sample initial states by taking random number of no-ops on reset.\"\"\"\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        super().__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        if self.noop_max > 0:\n",
    "            noops = np.random.randint(1, self.noop_max + 1)\n",
    "            for _ in range(noops):\n",
    "                obs, _, terminated, truncated, _ = self.env.step(self.noop_action)\n",
    "                if terminated or truncated:\n",
    "                    obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"Return only every `skip`-th frame, max-pooling across last 2 frames.\"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, term, trunc, inf = self.env.step(action)\n",
    "\n",
    "            # Always write the most recent observations; avoids stale frames if episode ends early\n",
    "            self._obs_buffer[i % 2] = obs\n",
    "\n",
    "            total_reward += reward\n",
    "            terminated = terminated or term\n",
    "            truncated = truncated or trunc\n",
    "            info = inf\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Warp frames to 84x84 grayscale as done in the Nature paper.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(self.height, self.width), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def observation(self, frame):\n",
    "        if len(frame.shape) == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "    def reward(self, reward):\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    \"\"\"Stack k last frames. Returns lazy array, which is much more memory efficient.\"\"\"\n",
    "    def __init__(self, env, k=4):\n",
    "        super().__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(k, *obs_shape), dtype=np.uint8\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs(), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return np.stack(self.frames, axis=0)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"Press FIRE button at the start of each episode to begin gameplay.\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Action 1 is typically FIRE in Atari games\n",
    "        # For Pong, we need to press FIRE to start the game\n",
    "        self.fire_action = 1\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Press FIRE to start the game\n",
    "        obs, _, terminated, truncated, info = self.env.step(self.fire_action)\n",
    "        if terminated or truncated:\n",
    "            # If game ended immediately, reset again\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "def make_atari_env(env_id='ALE/Pong-v5', max_episode_steps=108000, sticky_actions=False):\n",
    "    \"\"\"\n",
    "    Create and wrap Atari environment.\n",
    "\n",
    "    - Use ALE/Pong-v5 but force frameskip=1.\n",
    "    - Then MaxAndSkipEnv(skip=4) becomes the ONLY frameskip.\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        frameskip=1,  # critical: prevents 4x4=16 frame skipping\n",
    "        repeat_action_probability=0.0,\n",
    "        full_action_space=False,  # default is already reduced actions; keeping explicit\n",
    "    )\n",
    "\n",
    "    # (optional) keep your episode limit hack\n",
    "    env._max_episode_steps = max_episode_steps\n",
    "\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = FireResetEnv(env)  # ADD THIS - press FIRE to start game\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = WarpFrame(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = FrameStack(env, k=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "# Test the wrapper\n",
    "test_env = make_atari_env()\n",
    "test_obs, _ = test_env.reset()\n",
    "print(f\"âœ… Wrappers working! Observation shape: {test_obs.shape}, dtype: {test_obs.dtype}\")\n",
    "print(f\"   Action space: {test_env.action_space}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… Model working! Input shape: torch.Size([1, 4, 84, 84]), Output shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: DQN Network Model\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"DQN CNN network for Atari games.\"\"\"\n",
    "    def __init__(self, n_actions, n_frames=4):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Output size after conv layers: 7x7x64 = 3136\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalize to [0, 1]\n",
    "        x = x.float() / 255.0\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_model = DQNNetwork(n_actions=6, n_frames=4).to(device)\n",
    "test_input = torch.zeros(1, 4, 84, 84, dtype=torch.uint8).to(device)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"âœ… Model working! Input shape: {test_input.shape}, Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Replay Buffer ready (Memory-Optimized)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Replay Buffer (Memory-Optimized)\n",
    "\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Memory-optimized experience replay buffer for DQN.\n",
    "    Uses numpy arrays for efficient storage instead of deque.\n",
    "    \n",
    "    The capacity parameter acts as max_memory_length - when the buffer is full,\n",
    "    new entries automatically overwrite the oldest entries (circular buffer).\n",
    "    This prevents memory from growing unbounded.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=250000):  # capacity = max_memory_length (reduced from 1M to 250k to prevent MemoryError)\n",
    "        self.capacity = capacity\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "        \n",
    "        # Pre-allocate numpy arrays for efficient storage\n",
    "        # State shape: (4, 84, 84) uint8\n",
    "        self.states = np.zeros((capacity, 4, 84, 84), dtype=np.uint8)\n",
    "        self.next_states = np.zeros((capacity, 4, 84, 84), dtype=np.uint8)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int64)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add experience to buffer.\n",
    "        Automatically overwrites oldest entry when buffer is full (circular buffer).\n",
    "        This prevents memory from growing unbounded.\n",
    "        \"\"\"\n",
    "        self.states[self.ptr] = state\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.dones[self.ptr] = float(done)\n",
    "        \n",
    "        # Circular buffer: wrap around and overwrite oldest entry when full\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"Sample batch of experiences.\"\"\"\n",
    "        indices = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        # Use pre-allocated arrays for efficiency\n",
    "        states = torch.tensor(self.states[indices], dtype=torch.uint8, device=device)\n",
    "        actions = torch.tensor(self.actions[indices], dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(self.rewards[indices], dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(self.next_states[indices], dtype=torch.uint8, device=device)\n",
    "        dones = torch.tensor(self.dones[indices], dtype=torch.float32, device=device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "print(\"âœ… Replay Buffer ready (Memory-Optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DQN Agent ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: DQN Agent\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions,\n",
    "        device,\n",
    "        lr=6.25e-5,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        epsilon_decay_steps=1000000,\n",
    "        target_update_freq=10000,\n",
    "        buffer_size=1000000,\n",
    "        batch_size=32\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQNNetwork(n_actions).to(device)\n",
    "        self.target_network = DQNNetwork(n_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr, eps=1.5e-4)\n",
    "        \n",
    "        # Replay buffer (reduced size to prevent MemoryError)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Get current epsilon value using linear decay.\n",
    "        \n",
    "        This is the STANDARD approach used in DQN papers:\n",
    "        - Linear decay from epsilon_start to epsilon_end over epsilon_decay_steps\n",
    "        - Decay is based on training steps (not memory size)\n",
    "        - Formula: epsilon = start - (start - end) * (steps / decay_steps)\n",
    "        \n",
    "        Alternative approaches (NOT recommended):\n",
    "        1. Memory-based decay: epsilon -= (start-end)/MEMORY_SIZE\n",
    "           - Problem: Decay depends on buffer fill rate, not training progress\n",
    "           - Can decay too fast if buffer fills quickly\n",
    "        \n",
    "        2. Per-step decrement: epsilon -= interval / frames\n",
    "           - Problem: Requires storing epsilon as instance variable\n",
    "           - Less flexible than computed decay\n",
    "        \n",
    "        Current approach is optimal: deterministic, step-based, and matches DQN literature.\n",
    "        \"\"\"\n",
    "        if self.steps < self.epsilon_decay_steps:\n",
    "            # Linear decay: goes from start to end over decay_steps\n",
    "            epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * (self.steps / self.epsilon_decay_steps)\n",
    "            return max(epsilon, self.epsilon_end)\n",
    "        return self.epsilon_end\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.get_epsilon():\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.uint8, device=self.device).unsqueeze(0)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "        self.steps += 1\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(\n",
    "            self.batch_size, self.device\n",
    "        )\n",
    "        \n",
    "        # Compute Q-values\n",
    "        q_values = self.q_network(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            next_q_value = next_q_values.max(1)[0]\n",
    "            target_q_value = rewards + (1 - dones) * self.gamma * next_q_value\n",
    "        \n",
    "        # Compute loss and optimize\n",
    "        loss = F.smooth_l1_loss(q_value, target_q_value)\n",
    "\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load model.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps = checkpoint['steps']\n",
    "\n",
    "\n",
    "print(\"âœ… DQN Agent ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting training for 5,000,000 steps\n",
      "   Environment: Pong\n",
      "   Actions: 6\n",
      "   Device: cuda\n",
      "\n",
      "ðŸ“Š Memory Safety Measures:\n",
      "   âœ… Replay Buffer: 250,000 capacity (circular, bounded)\n",
      "   âœ… Episode History: 10,000 max episodes (auto-pop)\n",
      "   âœ… All data structures are memory-bounded\n",
      "\n",
      "ðŸŽ¯ Epsilon Decay Strategy:\n",
      "   âœ… Linear decay: 1.00 â†’ 0.10 over 250,000 steps\n",
      "   âœ… Step-based (not memory-based) - matches DQN literature\n",
      "   âœ… Deterministic and predictable decay schedule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 50080/5000000 [00:50<1:43:29, 797.16it/s, epsilon=0.819, loss=0.0160, avg_reward=-20.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 50,000:\n",
      "   Avg reward (last 10): -20.70\n",
      "   Avg length: 880.0\n",
      "   Epsilon: 0.820\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–         | 100114/5000000 [01:53<1:44:29, 781.52it/s, epsilon=0.639, loss=0.0050, avg_reward=-20.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 100,000:\n",
      "   Avg reward (last 10): -20.70\n",
      "   Avg length: 932.9\n",
      "   Epsilon: 0.640\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|â–Ž         | 150078/5000000 [03:00<1:56:02, 696.57it/s, epsilon=0.459, loss=0.0048, avg_reward=-20.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 150,000:\n",
      "   Avg reward (last 10): -20.20\n",
      "   Avg length: 951.7\n",
      "   Epsilon: 0.460\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–         | 200128/5000000 [04:09<2:00:14, 665.32it/s, epsilon=0.280, loss=0.0033, avg_reward=-19.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_200k.pth\n",
      "\n",
      "ðŸ“Š Step 200,000:\n",
      "   Avg reward (last 10): -19.60\n",
      "   Avg length: 1047.3\n",
      "   Epsilon: 0.280\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|â–Œ         | 250103/5000000 [05:23<2:00:07, 659.04it/s, epsilon=0.100, loss=0.0050, avg_reward=-19.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 250,000:\n",
      "   Avg reward (last 10): -19.00\n",
      "   Avg length: 1105.1\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–Œ         | 300115/5000000 [06:38<1:54:59, 681.18it/s, epsilon=0.100, loss=0.0067, avg_reward=-19.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 300,000:\n",
      "   Avg reward (last 10): -19.40\n",
      "   Avg length: 1177.9\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|â–‹         | 350104/5000000 [07:49<1:53:15, 684.25it/s, epsilon=0.100, loss=0.0033, avg_reward=-19.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 350,000:\n",
      "   Avg reward (last 10): -19.10\n",
      "   Avg length: 1224.7\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–Š         | 400100/5000000 [09:07<2:09:15, 593.11it/s, epsilon=0.100, loss=0.0035, avg_reward=-17.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_400k.pth\n",
      "\n",
      "ðŸ“Š Step 400,000:\n",
      "   Avg reward (last 10): -17.80\n",
      "   Avg length: 1406.1\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|â–‰         | 450096/5000000 [10:30<2:02:51, 617.26it/s, epsilon=0.100, loss=0.0065, avg_reward=-17.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 450,000:\n",
      "   Avg reward (last 10): -17.40\n",
      "   Avg length: 1511.9\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆ         | 500068/5000000 [11:52<2:00:32, 622.17it/s, epsilon=0.100, loss=0.0069, avg_reward=-17.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 500,000:\n",
      "   Avg reward (last 10): -17.50\n",
      "   Avg length: 1489.3\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|â–ˆ         | 550063/5000000 [13:13<1:59:24, 621.11it/s, epsilon=0.100, loss=0.0070, avg_reward=-17.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 550,000:\n",
      "   Avg reward (last 10): -17.30\n",
      "   Avg length: 1579.9\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|â–ˆâ–        | 600130/5000000 [14:33<1:58:09, 620.59it/s, epsilon=0.100, loss=0.0065, avg_reward=-17.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_600k.pth\n",
      "\n",
      "ðŸ“Š Step 600,000:\n",
      "   Avg reward (last 10): -17.20\n",
      "   Avg length: 1644.2\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|â–ˆâ–Ž        | 650124/5000000 [15:51<1:53:38, 637.97it/s, epsilon=0.100, loss=0.0088, avg_reward=-17.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 650,000:\n",
      "   Avg reward (last 10): -17.50\n",
      "   Avg length: 1682.6\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–        | 700076/5000000 [17:10<1:51:19, 643.78it/s, epsilon=0.100, loss=0.0179, avg_reward=-17.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 700,000:\n",
      "   Avg reward (last 10): -17.50\n",
      "   Avg length: 1591.5\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–Œ        | 750068/5000000 [18:29<1:50:39, 640.07it/s, epsilon=0.100, loss=0.0067, avg_reward=-17.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 750,000:\n",
      "   Avg reward (last 10): -17.00\n",
      "   Avg length: 1635.5\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–Œ        | 800080/5000000 [19:47<2:07:10, 550.39it/s, epsilon=0.100, loss=0.0035, avg_reward=-15.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_800k.pth\n",
      "\n",
      "ðŸ“Š Step 800,000:\n",
      "   Avg reward (last 10): -15.00\n",
      "   Avg length: 1799.5\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|â–ˆâ–‹        | 850098/5000000 [21:06<1:41:24, 682.00it/s, epsilon=0.100, loss=0.0085, avg_reward=-14.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 850,000:\n",
      "   Avg reward (last 10): -14.90\n",
      "   Avg length: 1938.6\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–Š        | 900075/5000000 [22:20<1:45:42, 646.41it/s, epsilon=0.100, loss=0.0039, avg_reward=-13.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 900,000:\n",
      "   Avg reward (last 10): -13.10\n",
      "   Avg length: 1955.4\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|â–ˆâ–‰        | 950115/5000000 [23:39<1:51:47, 603.74it/s, epsilon=0.100, loss=0.0083, avg_reward=-13.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 950,000:\n",
      "   Avg reward (last 10): -13.30\n",
      "   Avg length: 2097.7\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆ        | 1000116/5000000 [24:55<1:52:32, 592.37it/s, epsilon=0.100, loss=0.0075, avg_reward=-12.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_1000k.pth\n",
      "\n",
      "ðŸ“Š Step 1,000,000:\n",
      "   Avg reward (last 10): -12.50\n",
      "   Avg length: 2133.5\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|â–ˆâ–ˆ        | 1050097/5000000 [26:16<1:47:52, 610.22it/s, epsilon=0.100, loss=0.0096, avg_reward=-13.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,050,000:\n",
      "   Avg reward (last 10): -13.40\n",
      "   Avg length: 1977.4\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|â–ˆâ–ˆâ–       | 1100088/5000000 [27:38<2:00:38, 538.78it/s, epsilon=0.100, loss=0.0034, avg_reward=-12.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,100,000:\n",
      "   Avg reward (last 10): -12.50\n",
      "   Avg length: 2102.3\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|â–ˆâ–ˆâ–Ž       | 1150086/5000000 [29:00<1:45:18, 609.34it/s, epsilon=0.100, loss=0.0026, avg_reward=-11.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,150,000:\n",
      "   Avg reward (last 10): -11.70\n",
      "   Avg length: 2154.7\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|â–ˆâ–ˆâ–       | 1200088/5000000 [30:25<1:45:36, 599.71it/s, epsilon=0.100, loss=0.0078, avg_reward=-11.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_1200k.pth\n",
      "\n",
      "ðŸ“Š Step 1,200,000:\n",
      "   Avg reward (last 10): -11.50\n",
      "   Avg length: 2343.2\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|â–ˆâ–ˆâ–Œ       | 1250086/5000000 [31:46<1:41:32, 615.46it/s, epsilon=0.100, loss=0.0063, avg_reward=-11.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,250,000:\n",
      "   Avg reward (last 10): -11.80\n",
      "   Avg length: 2410.2\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 1300064/5000000 [33:08<1:40:45, 612.05it/s, epsilon=0.100, loss=0.0052, avg_reward=-12.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,300,000:\n",
      "   Avg reward (last 10): -12.40\n",
      "   Avg length: 2141.2\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|â–ˆâ–ˆâ–‹       | 1350074/5000000 [34:26<1:35:13, 638.82it/s, epsilon=0.100, loss=0.0051, avg_reward=-10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,350,000:\n",
      "   Avg reward (last 10): -10.20\n",
      "   Avg length: 2573.5\n",
      "   Epsilon: 0.100\n",
      "   â³ Still exploring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|â–ˆâ–ˆâ–Š       | 1400115/5000000 [35:46<1:39:37, 602.24it/s, epsilon=0.100, loss=0.0032, avg_reward=-8.0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_1400k.pth\n",
      "\n",
      "ðŸ“Š Step 1,400,000:\n",
      "   Avg reward (last 10): -8.00\n",
      "   Avg length: 2614.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|â–ˆâ–ˆâ–‰       | 1450092/5000000 [37:08<1:36:20, 614.15it/s, epsilon=0.100, loss=0.0043, avg_reward=-9.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,450,000:\n",
      "   Avg reward (last 10): -9.70\n",
      "   Avg length: 2534.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆ       | 1500064/5000000 [38:31<1:38:22, 593.00it/s, epsilon=0.100, loss=0.0029, avg_reward=-7.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,500,000:\n",
      "   Avg reward (last 10): -7.60\n",
      "   Avg length: 2763.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|â–ˆâ–ˆâ–ˆ       | 1550120/5000000 [39:53<1:32:43, 620.06it/s, epsilon=0.100, loss=0.0024, avg_reward=-8.0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,550,000:\n",
      "   Avg reward (last 10): -8.00\n",
      "   Avg length: 2748.3\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 1600104/5000000 [41:12<1:46:02, 534.39it/s, epsilon=0.100, loss=0.0075, avg_reward=-8.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_1600k.pth\n",
      "\n",
      "ðŸ“Š Step 1,600,000:\n",
      "   Avg reward (last 10): -8.30\n",
      "   Avg length: 2798.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1650116/5000000 [42:30<1:27:13, 640.07it/s, epsilon=0.100, loss=0.0094, avg_reward=-9.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,650,000:\n",
      "   Avg reward (last 10): -9.30\n",
      "   Avg length: 2672.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1700113/5000000 [43:48<1:27:57, 625.24it/s, epsilon=0.100, loss=0.0049, avg_reward=-7.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,700,000:\n",
      "   Avg reward (last 10): -7.00\n",
      "   Avg length: 3184.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1750108/5000000 [45:08<1:23:46, 646.49it/s, epsilon=0.100, loss=0.0039, avg_reward=-7.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,750,000:\n",
      "   Avg reward (last 10): -7.40\n",
      "   Avg length: 2955.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1800095/5000000 [46:26<1:27:35, 608.88it/s, epsilon=0.100, loss=0.0082, avg_reward=-7.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_1800k.pth\n",
      "\n",
      "ðŸ“Š Step 1,800,000:\n",
      "   Avg reward (last 10): -7.40\n",
      "   Avg length: 2832.4\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1850112/5000000 [47:53<1:24:23, 622.09it/s, epsilon=0.100, loss=0.0067, avg_reward=-6.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,850,000:\n",
      "   Avg reward (last 10): -6.40\n",
      "   Avg length: 2890.2\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1900116/5000000 [49:15<1:22:56, 622.95it/s, epsilon=0.100, loss=0.0143, avg_reward=-5.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,900,000:\n",
      "   Avg reward (last 10): -5.80\n",
      "   Avg length: 2897.2\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1950084/5000000 [50:37<1:21:05, 626.90it/s, epsilon=0.100, loss=0.0041, avg_reward=-2.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 1,950,000:\n",
      "   Avg reward (last 10): -2.30\n",
      "   Avg length: 3158.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2000091/5000000 [51:59<1:29:35, 558.05it/s, epsilon=0.100, loss=0.0026, avg_reward=-1.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_2000k.pth\n",
      "\n",
      "ðŸ“Š Step 2,000,000:\n",
      "   Avg reward (last 10): -1.80\n",
      "   Avg length: 3435.7\n",
      "   Epsilon: 0.100\n",
      "   ðŸ“ˆ Agent is learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2050131/5000000 [53:18<1:18:00, 630.29it/s, epsilon=0.100, loss=0.0034, avg_reward=0.4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,050,000:\n",
      "   Avg reward (last 10): 0.40\n",
      "   Avg length: 3138.7\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2100070/5000000 [54:39<1:20:20, 601.53it/s, epsilon=0.100, loss=0.0022, avg_reward=2.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,100,000:\n",
      "   Avg reward (last 10): 2.10\n",
      "   Avg length: 3119.8\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 2150075/5000000 [56:00<1:09:26, 684.05it/s, epsilon=0.100, loss=0.0168, avg_reward=0.9] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,150,000:\n",
      "   Avg reward (last 10): 0.90\n",
      "   Avg length: 3120.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2200096/5000000 [57:21<1:14:47, 623.93it/s, epsilon=0.100, loss=0.0076, avg_reward=5.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_2200k.pth\n",
      "\n",
      "ðŸ“Š Step 2,200,000:\n",
      "   Avg reward (last 10): 5.70\n",
      "   Avg length: 2967.4\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2250086/5000000 [58:36<1:07:50, 675.56it/s, epsilon=0.100, loss=0.0026, avg_reward=5.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,250,000:\n",
      "   Avg reward (last 10): 5.90\n",
      "   Avg length: 2900.3\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2300122/5000000 [59:49<1:11:59, 625.10it/s, epsilon=0.100, loss=0.0032, avg_reward=4.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,300,000:\n",
      "   Avg reward (last 10): 4.70\n",
      "   Avg length: 2837.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2350092/5000000 [1:01:12<1:10:58, 622.28it/s, epsilon=0.100, loss=0.0089, avg_reward=10.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,350,000:\n",
      "   Avg reward (last 10): 10.90\n",
      "   Avg length: 2588.5\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 2400085/5000000 [1:02:34<1:12:55, 594.14it/s, epsilon=0.100, loss=0.0036, avg_reward=12.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_2400k.pth\n",
      "\n",
      "ðŸ“Š Step 2,400,000:\n",
      "   Avg reward (last 10): 12.70\n",
      "   Avg length: 2343.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2450078/5000000 [1:03:54<1:05:46, 646.08it/s, epsilon=0.100, loss=0.0127, avg_reward=11.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,450,000:\n",
      "   Avg reward (last 10): 11.30\n",
      "   Avg length: 2616.7\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2500108/5000000 [1:05:12<1:07:54, 613.60it/s, epsilon=0.100, loss=0.0029, avg_reward=10.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,500,000:\n",
      "   Avg reward (last 10): 10.60\n",
      "   Avg length: 2624.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2550074/5000000 [1:06:33<1:06:28, 614.25it/s, epsilon=0.100, loss=0.0437, avg_reward=12.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,550,000:\n",
      "   Avg reward (last 10): 12.90\n",
      "   Avg length: 2443.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2600072/5000000 [1:07:54<1:07:38, 591.39it/s, epsilon=0.100, loss=0.0029, avg_reward=12.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_2600k.pth\n",
      "\n",
      "ðŸ“Š Step 2,600,000:\n",
      "   Avg reward (last 10): 12.10\n",
      "   Avg length: 2467.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 2650102/5000000 [1:09:15<1:02:33, 626.00it/s, epsilon=0.100, loss=0.0034, avg_reward=12.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,650,000:\n",
      "   Avg reward (last 10): 12.40\n",
      "   Avg length: 2363.5\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2700116/5000000 [1:10:37<1:00:50, 629.95it/s, epsilon=0.100, loss=0.0109, avg_reward=13.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,700,000:\n",
      "   Avg reward (last 10): 13.10\n",
      "   Avg length: 2371.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2750077/5000000 [1:11:55<58:23, 642.16it/s, epsilon=0.100, loss=0.0021, avg_reward=11.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,750,000:\n",
      "   Avg reward (last 10): 11.10\n",
      "   Avg length: 2579.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2800065/5000000 [1:13:14<1:08:42, 533.68it/s, epsilon=0.100, loss=0.0048, avg_reward=11.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_2800k.pth\n",
      "\n",
      "ðŸ“Š Step 2,800,000:\n",
      "   Avg reward (last 10): 11.20\n",
      "   Avg length: 2575.5\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2850102/5000000 [1:14:35<56:17, 636.48it/s, epsilon=0.100, loss=0.0042, avg_reward=11.6]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,850,000:\n",
      "   Avg reward (last 10): 11.60\n",
      "   Avg length: 2438.5\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2900062/5000000 [1:15:55<57:41, 606.73it/s, epsilon=0.100, loss=0.0097, avg_reward=12.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,900,000:\n",
      "   Avg reward (last 10): 12.00\n",
      "   Avg length: 2460.0\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2950107/5000000 [1:17:14<52:19, 652.85it/s, epsilon=0.100, loss=0.0033, avg_reward=12.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 2,950,000:\n",
      "   Avg reward (last 10): 12.50\n",
      "   Avg length: 2486.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3000120/5000000 [1:18:32<54:21, 613.25it/s, epsilon=0.100, loss=0.0034, avg_reward=12.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_3000k.pth\n",
      "\n",
      "ðŸ“Š Step 3,000,000:\n",
      "   Avg reward (last 10): 12.50\n",
      "   Avg length: 2364.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3050127/5000000 [1:19:50<50:35, 642.35it/s, epsilon=0.100, loss=0.0028, avg_reward=12.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,050,000:\n",
      "   Avg reward (last 10): 12.90\n",
      "   Avg length: 2435.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3100077/5000000 [1:21:09<49:56, 634.09it/s, epsilon=0.100, loss=0.0028, avg_reward=12.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,100,000:\n",
      "   Avg reward (last 10): 12.00\n",
      "   Avg length: 2501.1\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 3150116/5000000 [1:22:27<47:17, 651.88it/s, epsilon=0.100, loss=0.0045, avg_reward=12.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,150,000:\n",
      "   Avg reward (last 10): 12.20\n",
      "   Avg length: 2459.5\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 3200093/5000000 [1:23:47<59:56, 500.52it/s, epsilon=0.100, loss=0.0034, avg_reward=12.7]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_3200k.pth\n",
      "\n",
      "ðŸ“Š Step 3,200,000:\n",
      "   Avg reward (last 10): 12.70\n",
      "   Avg length: 2470.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3250072/5000000 [1:25:09<48:26, 602.17it/s, epsilon=0.100, loss=0.0027, avg_reward=12.9]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,250,000:\n",
      "   Avg reward (last 10): 12.90\n",
      "   Avg length: 2448.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3300085/5000000 [1:26:32<44:53, 631.03it/s, epsilon=0.100, loss=0.0054, avg_reward=14.7]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,300,000:\n",
      "   Avg reward (last 10): 14.70\n",
      "   Avg length: 2268.2\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3350081/5000000 [1:27:54<44:25, 619.01it/s, epsilon=0.100, loss=0.0027, avg_reward=13.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,350,000:\n",
      "   Avg reward (last 10): 13.70\n",
      "   Avg length: 2322.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 3400108/5000000 [1:29:15<45:28, 586.44it/s, epsilon=0.100, loss=0.0017, avg_reward=14.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_3400k.pth\n",
      "\n",
      "ðŸ“Š Step 3,400,000:\n",
      "   Avg reward (last 10): 14.00\n",
      "   Avg length: 2307.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3450089/5000000 [1:30:35<41:40, 619.96it/s, epsilon=0.100, loss=0.0016, avg_reward=14.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,450,000:\n",
      "   Avg reward (last 10): 14.30\n",
      "   Avg length: 2297.2\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3500071/5000000 [1:31:55<47:17, 528.55it/s, epsilon=0.100, loss=0.0034, avg_reward=14.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,500,000:\n",
      "   Avg reward (last 10): 14.10\n",
      "   Avg length: 2317.6\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3550108/5000000 [1:33:20<39:10, 616.78it/s, epsilon=0.100, loss=0.0026, avg_reward=14.0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,550,000:\n",
      "   Avg reward (last 10): 14.00\n",
      "   Avg length: 2196.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3600096/5000000 [1:34:41<39:05, 596.84it/s, epsilon=0.100, loss=0.0064, avg_reward=14.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Model saved: models/dqn_pong_3600k.pth\n",
      "\n",
      "ðŸ“Š Step 3,600,000:\n",
      "   Avg reward (last 10): 14.90\n",
      "   Avg length: 2241.0\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 3650097/5000000 [1:36:02<37:36, 598.31it/s, epsilon=0.100, loss=0.0031, avg_reward=13.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,650,000:\n",
      "   Avg reward (last 10): 11.70\n",
      "   Avg length: 2502.9\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 3700105/5000000 [1:37:21<32:02, 676.20it/s, epsilon=0.100, loss=0.0017, avg_reward=12.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,700,000:\n",
      "   Avg reward (last 10): 12.20\n",
      "   Avg length: 2426.0\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3750084/5000000 [1:38:38<34:59, 595.27it/s, epsilon=0.100, loss=0.0015, avg_reward=13.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Step 3,750,000:\n",
      "   Avg reward (last 10): 13.20\n",
      "   Avg length: 2363.3\n",
      "   Epsilon: 0.100\n",
      "   ðŸŽ‰ Agent is winning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3760919/5000000 [1:38:56<32:35, 633.57it/s, epsilon=0.100, loss=0.0017, avg_reward=14.9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.steps >= LEARN_START \u001b[38;5;129;01mand\u001b[39;00m agent.steps % TRAIN_FREQ == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     87\u001b[39m         \u001b[38;5;66;03m# Convert deque to list for numpy operations\u001b[39;00m\n\u001b[32m     88\u001b[39m         avg_reward = np.mean(\u001b[38;5;28mlist\u001b[39m(episode_rewards)[-\u001b[32m10\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(episode_rewards) >= \u001b[32m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mDQNAgent.train_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps % \u001b[38;5;28mself\u001b[39m.target_update_freq == \u001b[32m0\u001b[39m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.target_network.load_state_dict(\u001b[38;5;28mself\u001b[39m.q_network.state_dict())\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Loop\n",
    "\n",
    "# Hyperparameters\n",
    "TOTAL_STEPS = 5000000\n",
    "LEARN_START = 12500\n",
    "TRAIN_FREQ = 4\n",
    "SAVE_FREQ = 200000\n",
    "EVAL_FREQ = 50000\n",
    "\n",
    "# Create environment\n",
    "env = make_atari_env('ALE/Pong-v5')\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Create agent\n",
    "# Reduced buffer_size from 1M to 250k to prevent MemoryError\n",
    "# 250k is still sufficient for good performance and uses ~14GB instead of ~56GB\n",
    "agent = DQNAgent(\n",
    "    n_actions=n_actions,\n",
    "    device=device,\n",
    "    lr=6.25e-5,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay_steps=250000,\n",
    "    target_update_freq=10000,\n",
    "    buffer_size=250000,  # Reduced from 1M to prevent MemoryError\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Training statistics\n",
    "# Use deque with maxlen to automatically pop old history and prevent memory growth\n",
    "# Keep last 10000 episodes (should be more than enough for statistics)\n",
    "# This prevents unbounded memory growth during long training sessions\n",
    "MAX_EPISODE_HISTORY = 10000\n",
    "episode_rewards = deque(maxlen=MAX_EPISODE_HISTORY)\n",
    "episode_lengths = deque(maxlen=MAX_EPISODE_HISTORY)\n",
    "current_reward = 0\n",
    "current_length = 0\n",
    "\n",
    "# Memory safety summary:\n",
    "# âœ… Replay Buffer: Circular buffer with capacity=250k (~14GB fixed)\n",
    "# âœ… Episode Stats: deque with maxlen=10k (~80KB fixed)\n",
    "# âœ… FrameStack: deque with maxlen=4 (tiny, ~1KB)\n",
    "# âœ… All data structures are bounded and auto-pop old entries\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(f\"ðŸš€ Starting training for {TOTAL_STEPS:,} steps\")\n",
    "print(f\"   Environment: Pong\")\n",
    "print(f\"   Actions: {n_actions}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"\\nðŸ“Š Memory Safety Measures:\")\n",
    "print(f\"   âœ… Replay Buffer: {agent.replay_buffer.capacity:,} capacity (circular, bounded)\")\n",
    "print(f\"   âœ… Episode History: {MAX_EPISODE_HISTORY:,} max episodes (auto-pop)\")\n",
    "print(f\"   âœ… All data structures are memory-bounded\")\n",
    "print(f\"\\nðŸŽ¯ Epsilon Decay Strategy:\")\n",
    "print(f\"   âœ… Linear decay: {agent.epsilon_start:.2f} â†’ {agent.epsilon_end:.2f} over {agent.epsilon_decay_steps:,} steps\")\n",
    "print(f\"   âœ… Step-based (not memory-based) - matches DQN literature\")\n",
    "print(f\"   âœ… Deterministic and predictable decay schedule\")\n",
    "\n",
    "# Initialize state\n",
    "state, info = env.reset()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(TOTAL_STEPS), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "    # Select action\n",
    "    action = agent.select_action(state, training=True)\n",
    "    \n",
    "    # Take step\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Store transition\n",
    "    agent.store_transition(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Update statistics\n",
    "    current_reward += reward\n",
    "    current_length += 1\n",
    "    \n",
    "    # Train\n",
    "    if agent.steps >= LEARN_START and agent.steps % TRAIN_FREQ == 0:\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            # Convert deque to list for numpy operations\n",
    "            avg_reward = np.mean(list(episode_rewards)[-10:]) if len(episode_rewards) >= 10 else 0\n",
    "            pbar.set_postfix({\n",
    "                'epsilon': f'{agent.get_epsilon():.3f}',\n",
    "                'loss': f'{loss:.4f}',\n",
    "                'avg_reward': f'{avg_reward:.1f}' if len(episode_rewards) >= 10 else 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Handle episode end\n",
    "    if done:\n",
    "        episode_rewards.append(current_reward)\n",
    "        episode_lengths.append(current_length)\n",
    "        current_reward = 0\n",
    "        current_length = 0\n",
    "        state, info = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    # Save model\n",
    "    if step > 0 and step % SAVE_FREQ == 0:\n",
    "        model_path = f'models/dqn_pong_{step//1000}k.pth'\n",
    "        agent.save(model_path)\n",
    "        print(f\"\\nðŸ’¾ Model saved: {model_path}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    if step > 0 and step % EVAL_FREQ == 0 and len(episode_rewards) >= 10:\n",
    "        # Convert deque to list for numpy operations\n",
    "        rewards_list = list(episode_rewards)\n",
    "        lengths_list = list(episode_lengths)\n",
    "        avg_reward = np.mean(rewards_list[-10:])\n",
    "        avg_length = np.mean(lengths_list[-10:])\n",
    "        print(f\"\\nðŸ“Š Step {step:,}:\")\n",
    "        print(f\"   Avg reward (last 10): {avg_reward:.2f}\")\n",
    "        print(f\"   Avg length: {avg_length:.1f}\")\n",
    "        print(f\"   Epsilon: {agent.get_epsilon():.3f}\")\n",
    "        \n",
    "        if avg_reward > 0:\n",
    "            print(\"   ðŸŽ‰ Agent is winning!\")\n",
    "        elif avg_reward > -10:\n",
    "            print(\"   ðŸ“ˆ Agent is learning!\")\n",
    "        else:\n",
    "            print(\"   â³ Still exploring...\")\n",
    "\n",
    "# Final save\n",
    "agent.save('models/dqn_pong_final.pth')\n",
    "env.close()\n",
    "pbar.close()\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Total episodes recorded: {len(episode_rewards)}\")\n",
    "if len(episode_rewards) > 0:\n",
    "    # Convert deque to list for numpy operations\n",
    "    rewards_list = list(episode_rewards)\n",
    "    print(f\"Final avg reward (last 100): {np.mean(rewards_list[-100:]):.2f}\")\n",
    "    print(f\"Best episode: {max(rewards_list):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
